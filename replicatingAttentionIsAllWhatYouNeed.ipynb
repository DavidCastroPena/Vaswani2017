{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYH1Sflrb2J3yoHknRQd3P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidCastroPena/Vaswani2017/blob/main/replicatingAttentionIsAllWhatYouNeed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnreNdFpCsLR",
        "outputId": "e8e22952-6966-46d0-df82-e3b66bf7e4a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# when we are using a transformer to analyze say a sentence (\"the cat sat\"), first what happens is that each word gets stored in a\n",
        "# query: q_sat. Now each query or what i am looking to analize is compared (dot product) with its all the keys; all words have keys: k_the, k_cat,\n",
        "# and k_sat. The dot product geometrically shows that if the direction of two vectors is related, this will reflect a large dot\n",
        "#product, which shows that the two tokens are related\n",
        "\n",
        "\"\"\"\n",
        "qsatâ‹…kthe= 0.1\n",
        "\tâ€‹\n",
        "ð‘žsatâ‹…ð‘˜cat=2.1\n",
        "\n",
        "qsatâ‹…kcat=1.5\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# \"sat\" is more related to cat than the\n",
        "\n",
        "#Recall, the keys and query are linear transformation of their embeddings. Intuitively, we say that the query is a question\n",
        "#per token that aims to uncover the role of the specific word in a given text. The word â€œquestionâ€ is shorthand for\n",
        "#something very precise: The query defines a direction in vector space along which relevance is measured.\n",
        "\n",
        "\n",
        "\n",
        "#Lesson 1 goal: implement and understand:\n",
        "\n",
        "#Attention(Q,K,V)=softmax(QK^T/root(d_k)*V\n",
        "\n",
        "#Key concepts:\n",
        "\n",
        "#PyTorch tensors\n",
        "\n",
        "#Matrix multiplication\n",
        "\n",
        "#Softmax\n",
        "\n",
        "#Masking\n",
        "\n",
        "#Shape reasoning\n",
        "\n",
        "\n",
        "#Setup\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(0)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d79de55",
        "outputId": "e9b6a59c-3fdb-436c-f049-a113856a74bf"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Define dimensions for the SelfAttentionHead\n",
        "d_model = 64  # Embedding dimension\n",
        "d_k = 32      # Dimension of Query and Key vectors\n",
        "d_v = 32      # Dimension of Value vectors\n",
        "\n",
        "# Instantiate the SelfAttentionHead\n",
        "attention_head = SelfAttentionHead(d_model, d_k, d_v).to(device)\n",
        "\n",
        "# Create a dummy input tensor (batch_size, sequence_length, d_model)\n",
        "batch_size = 2\n",
        "sequence_length = 5\n",
        "X = torch.randn(batch_size, sequence_length, d_model, device=device)\n",
        "\n",
        "# Perform a forward pass\n",
        "out, attn = attention_head(X)\n",
        "\n",
        "print(\"Input X shape:\", X.shape)\n",
        "print(\"Output 'out' shape:\", out.shape)\n",
        "print(\"Attention 'attn' shape:\", attn.shape)\n",
        "\n",
        "# Verify row sums of attention matrix (should be ~1)\n",
        "print(\"\\nAttention matrix row sums (first batch, first head, should be ~1):\\n\", attn[0].sum(dim=-1).detach().cpu())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input X shape: torch.Size([2, 5, 64])\n",
            "Output 'out' shape: torch.Size([2, 5, 32])\n",
            "Attention 'attn' shape: torch.Size([2, 5, 5])\n",
            "\n",
            "Attention matrix row sums (first batch, first head, should be ~1):\n",
            " tensor([1., 1., 1., 1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Q,K ,V, and a mask are tensor. A tensor is a multi-dimensional array of numbers and in transformers all the meaning\n",
        "#arises from learned tensor transformations and interactions\n",
        "\n",
        "# embeddings are vectors, but to make computations or to represent sentences, instead of converting to\n",
        "# vectors in a sequential way or one by one, tensors allow you to represent text in a more efficient way\n",
        "\n",
        "\n",
        "# A vector is a tensor (a 1-D tensor). A matrix is a tensor (a 2-D tensor). So the real question is:\n",
        "\n",
        "# Why do we need higher-dimensional tensors instead of just one vector at a time? Imagine you process one word at a time,\n",
        "# using vectors only. Sentence: â€œThe cat satâ€.You would have to do this:\n",
        "\n",
        "# Take embedding of \"The\" â†’ vector compare it to \"cat\" â†’ vector\n",
        "\n",
        "# Compare it to \"sat\" â†’ vector\n",
        "\n",
        "# Repeat for \"cat\"\n",
        "\n",
        "# Repeat for \"sat\"\n",
        "\n",
        "# Thatâ€™s nested loopsnsequential computation very slow hard to parallelize messy gradients. This is basically how early RNNs worked.\n",
        "\n",
        "# What tensors give you: structure Tensors let you represent many things at once. Instead of:\n",
        "\n",
        "# â€œOne word â†’ one vectorâ€ You represent: All words, all positions, all heads, all batches â€” at the same time\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Q: (batch, heads, q_len, d_k)\n",
        "    K: (batch, heads, k_len, d_k)\n",
        "    V: (batch, heads, k_len, d_v)\n",
        "    mask: (batch, heads, q_len, k_len) with 1 for allowed, 0 for blocked (optional)\n",
        "\n",
        "    returns:\n",
        "      out: (batch, heads, q_len, d_v)\n",
        "      attn: (batch, heads, q_len, k_len)\n",
        "    \"\"\"\n",
        "    d_k = Q.size(-1)\n",
        "\n",
        "    # (batch, heads, q_len, k_len)\n",
        "\n",
        "  #When you compute qverb*kj, you are asking:\n",
        "  # \"does token k, in its current representation, lie in a directaion that is useful for a verb right now?\"\n",
        "  # if the answer is yes (large dot product), attention weight increases, information form that token flows\n",
        "\n",
        "\n",
        "    scores = Q @ K.transpose(-2, -1) / (d_k ** 0.5)\n",
        "\n",
        "    if mask is not None:\n",
        "        # Set blocked positions to a large negative value so softmax ~ 0 there.\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "#the softmax ensure converting the raw scores into probabilities of how related or important are\n",
        "#the tokens to each other. However, it does not have the information itself: recall that q\n",
        "#contains the question the word asks about its role, the K decide how to be matched, and V decide what\n",
        "# information to contribute.\n",
        "\n",
        "# Consider the word \"bank\". For matching, you may want to match syntactic role or entity type.\n",
        "# For content, you may want to pass financial or river meaning. In this case, Kj encodes:\n",
        "# â€œIf another token is looking for this kind of information, how strongly should I respond?â€ This response\n",
        "#is continuous, context-dependet, and learned statistically\n",
        "\n",
        "# Sentence â€œThe cat sat.â€ During training, the model repeatedly observes that: verbs benefit from attending to certain tokens\n",
        "# those tokens often share embedding patterns (things we humans call â€œnounsâ€)\n",
        "# Over time:\n",
        "\n",
        "  # Wk learns to map those tokens into a region of space\n",
        "  # that region is useful when queried by verbs\n",
        "\n",
        "#  learns to map those tokens into a region of space,that region is useful when queried by verbs\n",
        "\n",
        "# But the model never learns:\n",
        "\n",
        "# â€œThis is a nounâ€\n",
        "\n",
        "# It learns: â€œTokens with embeddings like this tend to be useful when verbs query the context.â€ Thatâ€™s a relational regularity, not a category.\n",
        "\n",
        "\n",
        "    attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "# # the multiplication between the attn and v: The multiplication of attention weights by values computes a learned,\n",
        "# conditional linear combination of representations â€” mathematically similar to a regression or mixture model â€”\n",
        "# but embedded inside a deep, nonlinear system.\n",
        "\n",
        "\n",
        "    out = attn @ V\n",
        "    return out, attn\n",
        "\n"
      ],
      "metadata": {
        "id": "0evCiQo3C6lY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets create a small example to see the attention at work\n",
        "\n",
        "batch, heads, seq_len, d_k, d_v = 1, 1, 4, 3, 2\n",
        "\n",
        "Q = torch.randn(batch, heads, seq_len, d_k, device=device)\n",
        "K = torch.randn(batch, heads, seq_len, d_k, device=device)\n",
        "V = torch.randn(batch, heads, seq_len, d_v, device=device)\n",
        "\n",
        "out, attn = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "print(\"Q shape:\", Q.shape)\n",
        "print(\"attn shape:\", attn.shape)\n",
        "print(\"out shape:\", out.shape)\n",
        "\n",
        "print(\"\\nAttention matrix (q_len x k_len):\\n\", attn[0,0].detach().cpu())\n",
        "print(\"\\nRow sums (should be ~1):\\n\", attn[0,0].sum(dim=-1).detach().cpu())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDKyBI-mL8VM",
        "outputId": "4c8c35c1-0575-4ac0-bc2a-5f57c2059a38"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q shape: torch.Size([1, 1, 4, 3])\n",
            "attn shape: torch.Size([1, 1, 4, 4])\n",
            "out shape: torch.Size([1, 1, 4, 2])\n",
            "\n",
            "Attention matrix (q_len x k_len):\n",
            " tensor([[0.3625, 0.2162, 0.2355, 0.1857],\n",
            "        [0.0161, 0.6331, 0.3333, 0.0176],\n",
            "        [0.0778, 0.0909, 0.1192, 0.7121],\n",
            "        [0.7052, 0.0087, 0.0229, 0.2631]])\n",
            "\n",
            "Row sums (should be ~1):\n",
            " tensor([1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“Œ NOTE â€” Bridging the gap: from scaled dot-product attention to a Self-Attention Head\n",
        "\n",
        "At this point in the replication, it is **normal to feel lost**.\n",
        "\n",
        "Why?  \n",
        "Because we moved from:\n",
        "- understanding **attention as a mathematical operation**\n",
        "to:\n",
        "- seeing **attention embedded inside a neural network module**\n",
        "\n",
        "This note explains **exactly how those two relate**, step by step.\n",
        "\n",
        "---\n",
        "\n",
        "## 1ï¸âƒ£ What we implemented first: scaled dot-product attention (pure math)\n",
        "\n",
        "We started by implementing:\n",
        "\n",
        "Attention(Q, K, V) = softmax(QKáµ€ / âˆšdâ‚–) Â· V\n",
        "\n",
        "This function:\n",
        "\n",
        "- takes **Q, K, V as inputs**\n",
        "- computes similarity scores via dot products\n",
        "- normalizes them with softmax\n",
        "- uses them to compute a weighted sum of values\n",
        "\n",
        "At this stage:\n",
        "- Q, K, V were **dummy tensors** (random numbers)\n",
        "- the goal was to understand:\n",
        "  - tensor shapes\n",
        "  - matrix multiplication\n",
        "  - softmax\n",
        "  - attention matrices\n",
        "  - how information flows\n",
        "\n",
        "This step was **intentional** and **necessary**.\n",
        "\n",
        "ðŸ‘‰ Key point:\n",
        "> `scaled_dot_product_attention` is a **standalone mathematical operator**.\n",
        "> It does NOT know where Q, K, V come from.\n",
        "\n",
        "Think of it as the **engine** of attention.\n",
        "\n",
        "---\n",
        "\n",
        "## 2ï¸âƒ£ The missing question: where do Q, K, V come from in a real Transformer?\n",
        "\n",
        "In the actual Transformer paper:\n",
        "\n",
        "- Q, K, V are **not given directly**\n",
        "- They are **computed from token embeddings**\n",
        "\n",
        "The real pipeline is:\n",
        "\n",
        "tokens  \n",
        "â†’ embeddings X  \n",
        "â†’ linear projections  \n",
        "â†’ Q, K, V  \n",
        "â†’ attention\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "Q = X Â· W_Q  \n",
        "K = X Â· W_K  \n",
        "V = X Â· W_V  \n",
        "\n",
        "Where:\n",
        "- X is the embedding tensor (B Ã— T Ã— d_model)\n",
        "- W_Q, W_K, W_V are **learned weight matrices**\n",
        "\n",
        "So Q, K, V are:\n",
        "- different *views* of the same tokens\n",
        "- learned automatically during training\n",
        "\n",
        "---\n",
        "\n",
        "## 3ï¸âƒ£ Why `SelfAttentionHead` exists\n",
        "\n",
        "The purpose of `SelfAttentionHead` is **not to introduce new math**.\n",
        "\n",
        "Its only job is to:\n",
        "\n",
        "1. Take token embeddings `X`\n",
        "2. Create Q, K, V using learned linear layers\n",
        "3. Call `scaled_dot_product_attention(Q, K, V)`\n",
        "4. Return:\n",
        "   - contextualized token representations\n",
        "   - the attention matrix\n",
        "\n",
        "Conceptually:\n",
        "\n",
        "SelfAttentionHead(X)\n",
        "    â”œâ”€â”€ computes Q = XW_Q\n",
        "    â”œâ”€â”€ computes K = XW_K\n",
        "    â”œâ”€â”€ computes V = XW_V\n",
        "    â””â”€â”€ calls scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "So:\n",
        "\n",
        "- `scaled_dot_product_attention` = the **math**\n",
        "- `SelfAttentionHead` = the **first real Transformer component**\n",
        "\n",
        "Nothing magical is happening here.\n",
        "\n",
        "---\n",
        "\n",
        "## 4ï¸âƒ£ Why new tensor dimensions suddenly appear (the â€œheadsâ€ dimension)\n",
        "\n",
        "The attention function is written to support **multi-head attention** later.\n",
        "\n",
        "For that reason, it expects tensors shaped like:\n",
        "\n",
        "(Batch, Heads, SequenceLength, Dimension)\n",
        "\n",
        "Even when:\n",
        "- batch = 1\n",
        "- heads = 1\n",
        "\n",
        "So inside `SelfAttentionHead`, we do:\n",
        "\n",
        "Q = Q.unsqueeze(1)\n",
        "\n",
        "This:\n",
        "- does NOT change the meaning of Q\n",
        "- only adds a placeholder dimension for heads\n",
        "- allows the same function to work for 1 head or many heads\n",
        "\n",
        "ðŸ‘‰ This is a **software design choice**, not a conceptual change.\n",
        "\n",
        "---\n",
        "\n",
        "## 5ï¸âƒ£ Why this step is part of the replication plan\n",
        "\n",
        "This step corresponds to **Section 3.2.1** of the paper:\n",
        "> â€œScaled Dot-Product Attentionâ€\n",
        "\n",
        "But now implemented as:\n",
        "- a **learnable module**\n",
        "- not just a formula on paper\n",
        "\n",
        "At this point in the replication, we are still in:\n",
        "\n",
        "âœ… Step 1 â€” Build attention correctly\n",
        "\n",
        "We are NOT yet:\n",
        "- stacking layers\n",
        "- using multiple heads\n",
        "- training a model\n",
        "- optimizing performance\n",
        "\n",
        "We are simply answering:\n",
        "\n",
        "> â€œHow does a Transformer turn embeddings into attention outputs?â€\n",
        "\n",
        "---\n",
        "\n",
        "## 6ï¸âƒ£ Key takeaway (lock this in)\n",
        "\n",
        "Nothing new was introduced conceptually.\n",
        "\n",
        "We only:\n",
        "- connected the attention math\n",
        "- to learnable parameters\n",
        "- in a reusable module\n",
        "\n",
        "If you understand:\n",
        "- how scaled dot-product attention works\n",
        "- and that `SelfAttentionHead` just *creates Q, K, V*\n",
        "\n",
        "then you understand this part of the Transformer.\n",
        "\n",
        "---\n",
        "\n",
        "## 7ï¸âƒ£ What comes next (but not yet)\n",
        "\n",
        "Only AFTER this is fully clear do we move on to:\n",
        "- masking (causal attention)\n",
        "- multi-head attention\n",
        "- stacking layers\n",
        "\n",
        "We are still building the foundation.\n"
      ],
      "metadata": {
        "id": "ow7_fZ-NCx8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Explaining the  make causal mask: attn[i, j] = how much token i (the query) attends to token j (the key/value).\n",
        "\n",
        "# Because you used a causal mask, token i is only allowed to attend to tokens â‰¤ i (no looking ahead).\n",
        "\n",
        "# Thatâ€™s why everything above the diagonal is zero.\n",
        "\n",
        "def make_causal_mask(B, H, T, device):\n",
        "    # lower-triangular matrix: 1 = allowed, 0 = blocked\n",
        "    m = torch.tril(torch.ones(T, T, device=device))\n",
        "    # expand to (B, H, T, T)\n",
        "    return m.view(1, 1, T, T).expand(B, H, T, T)\n"
      ],
      "metadata": {
        "id": "crV8l9UwEmLG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Testing the causal masking: torch.manual_seed(0)\n",
        "# Interpreting results:\n",
        "# [0.5445, 0.4555, 0.0000, 0.0000]\n",
        "# ~54% to token 0\n",
        "\n",
        "# Exactly expected.\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "B, T, d_model, d_k, d_v = 1, 4, 8, 4, 4\n",
        "X = torch.randn(B, T, d_model, device=device)\n",
        "\n",
        "head = SelfAttentionHead(d_model, d_k, d_v).to(device)\n",
        "\n",
        "mask = make_causal_mask(B, 1, T, device)  # (B, H=1, T, T)\n",
        "\n",
        "out, attn = head(X, mask=mask)\n",
        "\n",
        "print(\"attn:\\n\", attn[0].detach().cpu())\n",
        "print(\"row sums:\\n\", attn[0].sum(dim=-1).detach().cpu())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LepUvHp5LepZ",
        "outputId": "ad25cc37-bb05-4da3-d068-6c05f15a21ae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attn:\n",
            " tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5559, 0.4441, 0.0000, 0.0000],\n",
            "        [0.2983, 0.2798, 0.4219, 0.0000],\n",
            "        [0.1572, 0.2115, 0.3404, 0.2909]])\n",
            "row sums:\n",
            " tensor([1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# This class implements a SINGLE self-attention head: he SelfAttentionHead class, which inherits from nn.Module,\n",
        "# represents a concrete component within a neural network. Its primary responsibility is to create the Q, K, and V tensors from the input embeddings X using learnable linear projections (Wq, Wk, Wv).\n",
        "# Why it's a class (inheriting from nn.Module): Classes inheriting from nn.Module in PyTorch are designed for components that:\n",
        "# Contain Learnable Parameters: The Wq, Wk, and Wv matrices are nn.Linear layers, which are learnable. nn.Module manages these parameters, making them discoverable by optimizers during training.\n",
        "# Define a forward pass: The forward method encapsulates the sequence of operations (creating Q/K/V, calling scaled_dot_product_attention, handling dimensions) that this specific module performs.\n",
        "# Are part of a larger graph: By being an nn.Module, it can be easily integrated into a larger neural network (like a full Transformer block), allowing for proper dependency tracking and automatic differentiation.\n",
        "\n",
        "class SelfAttentionHead(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, d_k: int, d_v: int):\n",
        "        super().__init__()\n",
        "\n",
        "# At this point, we are passing from embeddings per each token to the matrices WQ,WK, and V learned matrices.\n",
        "# The learned\n",
        "        # Linear projection that maps embeddings -> query vectors\n",
        "        # Wq is a learned matrix of shape (d_model, d_k)\n",
        "        self.Wq = nn.Linear(d_model, d_k, bias=False)\n",
        "\n",
        "        # Linear projection that maps embeddings -> key vectors\n",
        "        # Wk is a learned matrix of shape (d_model, d_k)\n",
        "        self.Wk = nn.Linear(d_model, d_k, bias=False)\n",
        "\n",
        "        # Linear projection that maps embeddings -> value vectors\n",
        "        # Wv is a learned matrix of shape (d_model, d_v)\n",
        "        self.Wv = nn.Linear(d_model, d_v, bias=False)\n",
        "#X: (B, T, d_model) which is a fundamental way we describe tensors in deep learning, especially when working with sequences\n",
        "# like text. Let's break down each part:X: This is the input tensor itself. In the context of a Transformer,\n",
        "# it usually represents a batch of token embeddings.\n",
        "\n",
        "# B (Batch Size):\n",
        "\n",
        "# This is the number of independent sequences (e.g., sentences, phrases, or documents) that are being processed together.\n",
        "# Modern neural networks, especially when using GPUs, process data in 'batches' rather than one item at a time.\n",
        "#This is much more efficient because it allows for parallel computations.\n",
        "# Example: If B=3, it means you're feeding three different sentences into the model simultaneously.\n",
        "\n",
        "#T (Sequence Length / Number of Tokens):\n",
        "\n",
        "# This is the length of each sequence in the batch. It represents how many tokens (words, sub-words, or characters,\n",
        "# depending on your tokenization) are in each individual input sequence.\n",
        "# Example: If T=10, it means each of your B sentences has 10 tokens.\n",
        "# d_model (Model Dimension / Embedding Dimension):\n",
        "\n",
        "# This is the dimensionality of the vector used to represent each single token. Every token (like a word) is\n",
        "#converted into a numerical vector of this size. This vector is meant to capture the semantic and positional\n",
        "#information of that token. Example: If d_model=512, then each token in your T length sequences is represented\n",
        "#by a vector of 512 numbers.\n",
        "\n",
        "    def forward(self, X, mask=None):\n",
        "        \"\"\"\n",
        "        X: (B, T, d_model)\n",
        "        returns:\n",
        "          out:  (B, T, d_v)\n",
        "          attn: (B, T, T)\n",
        "        \"\"\"\n",
        "\n",
        "        # Project embeddings -> Q/K/V\n",
        "        Q = self.Wq(X)  # (B, T, d_k)\n",
        "        K = self.Wk(X)  # (B, T, d_k)\n",
        "        V = self.Wv(X)  # (B, T, d_v)\n",
        "\n",
        "        # Add head dimension so we can reuse the same attention function\n",
        "        Q = Q.unsqueeze(1)  # (B, 1, T, d_k)\n",
        "        K = K.unsqueeze(1)  # (B, 1, T, d_k)\n",
        "        V = V.unsqueeze(1)  # (B, 1, T, d_v)\n",
        "\n",
        "        out, attn = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
        "\n",
        "        # Remove head dimension (since H=1 here)\n",
        "        return out.squeeze(1), attn.squeeze(1)"
      ],
      "metadata": {
        "id": "Qlo4JRLxW6ps"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“Œ NOTE â€” The correct mental model: architecture vs learning in the Transformer\n",
        "\n",
        "At this point, it is important to separate **what is being built** from **what is being learned**.\n",
        "\n",
        "The Transformer paper combines mathematics, architecture, and training in one narrative.\n",
        "In this notebook, we are intentionally **separating them**, which is why it may feel like\n",
        "â€œnothing is being learned yetâ€.\n",
        "\n",
        "This note clarifies the correct mental model.\n",
        "\n",
        "---\n",
        "\n",
        "## 1ï¸âƒ£ Three distinct layers of understanding (do not mix them)\n",
        "\n",
        "When reading *Attention Is All You Need*, there are **three different layers** happening at once:\n",
        "\n",
        "---\n",
        "\n",
        "### Layer 1 â€” The mathematical operator (pure attention math)\n",
        "\n",
        "This is the formula:\n",
        "\n",
        "Attention(Q, K, V) = softmax(QKáµ€ / âˆšdâ‚–) Â· V\n",
        "\n",
        "At this layer:\n",
        "- Q, K, V are just tensors\n",
        "- no corpus is required\n",
        "- no learning is required\n",
        "- this is pure linear algebra\n",
        "\n",
        "This is what we implemented first with:\n",
        "- dummy Q, K, V\n",
        "- random tensors\n",
        "- shape checks\n",
        "- attention matrices\n",
        "\n",
        "ðŸ‘‰ Purpose: understand **how attention works mechanically**\n",
        "\n",
        "---\n",
        "\n",
        "### Layer 2 â€” The architectural module (parameterized attention)\n",
        "\n",
        "This is where `SelfAttentionHead` appears.\n",
        "\n",
        "At this layer:\n",
        "- we introduce **learnable matrices** W_Q, W_K, W_V\n",
        "- Q, K, V are computed as:\n",
        "  \n",
        "  Q = X Â· W_Q  \n",
        "  K = X Â· W_K  \n",
        "  V = X Â· W_V  \n",
        "\n",
        "- X are token embeddings\n",
        "- W_Q, W_K, W_V are **trainable parameters**\n",
        "\n",
        "Important:\n",
        "- these matrices are **randomly initialized**\n",
        "- they are *capable* of learning\n",
        "- but they have **not learned anything yet**\n",
        "\n",
        "ðŸ‘‰ Purpose: build the **Transformer architecture**, not train it\n",
        "\n",
        "This corresponds to the *architecture description* in the paper (Section 3).\n",
        "\n",
        "---\n",
        "\n",
        "### Layer 3 â€” Training (where learning actually happens)\n",
        "\n",
        "Learning only happens when we introduce:\n",
        "\n",
        "- a dataset (toy task or real text corpus)\n",
        "- a prediction objective (e.g., next-token prediction)\n",
        "- a loss function\n",
        "- backpropagation\n",
        "- an optimizer\n",
        "\n",
        "Only then do:\n",
        "- W_Q, W_K, W_V\n",
        "- embeddings\n",
        "- all other parameters\n",
        "\n",
        "begin to change in response to data.\n",
        "\n",
        "ðŸ‘‰ Purpose: make attention **meaningful**\n",
        "\n",
        "This corresponds to the *training procedure* sections of the paper.\n",
        "\n",
        "---\n",
        "\n",
        "## 2ï¸âƒ£ What we are doing right now in this notebook\n",
        "\n",
        "We are currently **between Layer 1 and Layer 2**.\n",
        "\n",
        "Specifically:\n",
        "- we fully understand the attention math (Layer 1)\n",
        "- we have built the parameterized attention module (Layer 2)\n",
        "- we have NOT introduced data, loss, or optimization (Layer 3)\n",
        "\n",
        "So it is **expected and correct** that:\n",
        "- W_Q, W_K, W_V exist\n",
        "- but they do not encode syntax, semantics, or grammar\n",
        "- attention patterns look random\n",
        "\n",
        "This is not a bug.\n",
        "This is the correct state at this stage.\n",
        "\n",
        "---\n",
        "\n",
        "## 3ï¸âƒ£ Why we are not using a corpus yet\n",
        "\n",
        "We intentionally do NOT start with real text because:\n",
        "\n",
        "- bugs in attention math are hard to debug once training is added\n",
        "- shape errors become opaque\n",
        "- attention feels like â€œmagicâ€ instead of computation\n",
        "\n",
        "The correct replication order is:\n",
        "\n",
        "1. Build attention\n",
        "2. Verify attention\n",
        "3. Build the Transformer block\n",
        "4. Train on **toy tasks**\n",
        "5. Only then train on real text\n",
        "\n",
        "This is exactly the plan we are following.\n",
        "\n",
        "---\n",
        "\n",
        "## 4ï¸âƒ£ How the paperâ€™s wording can be misleading\n",
        "\n",
        "When the paper says:\n",
        "> â€œWe use learned linear projections to obtain the queries, keys, and valuesâ€¦â€\n",
        "\n",
        "They are describing the **final trained system**, not the initial state of the code.\n",
        "\n",
        "In code:\n",
        "- those projections exist immediately\n",
        "- but they only become â€œlearnedâ€ after training\n",
        "\n",
        "---\n",
        "\n",
        "## 5ï¸âƒ£ Key takeaway (lock this in)\n",
        "\n",
        "> **Right now, we are building the machine.  \n",
        "> Learning only begins once we add a dataset, a loss, and optimization.**\n",
        "\n",
        "If you understand:\n",
        "- how attention works mathematically\n",
        "- how Q/K/V are produced architecturally\n",
        "- and why learning has not started yet\n",
        "\n",
        "then you have the correct mental model.\n",
        "\n",
        "---\n",
        "\n",
        "## 6ï¸âƒ£ What comes next\n",
        "\n",
        "Before touching real text, we will:\n",
        "- finish attention (masking)\n",
        "- add multi-head attention\n",
        "- stack layers\n",
        "- then train on simple toy tasks\n",
        "\n",
        "Only after that will we introduce a real corpus.\n",
        "\n",
        "This is the correct and disciplined way to replicate the paper.\n"
      ],
      "metadata": {
        "id": "EQA0qm8BDwhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Attention (MHA)\n",
        "\n",
        "    Input:\n",
        "      X: (B, T, d_model)\n",
        "\n",
        "    Output:\n",
        "      out:  (B, T, d_model)\n",
        "      attn: (B, h, T, T)   # attention matrix per head\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.h = num_heads\n",
        "        self.d_k = d_model // num_heads  # dimension per head (queries/keys/values per head)\n",
        "\n",
        "        # Big projections: X -> Q,K,V all in d_model space\n",
        "        # Each contains all heads packed together.\n",
        "        self.Wq = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.Wk = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.Wv = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        # Final output projection: concat(heads) -> d_model\n",
        "        self.Wo = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    def forward(self, X, mask=None):\n",
        "        \"\"\"\n",
        "        X: (B, T, d_model)\n",
        "\n",
        "        mask: should be broadcastable to (B, h, T, T)\n",
        "              - for causal masking you can build (B, 1, T, T) and it will broadcast across heads\n",
        "        \"\"\"\n",
        "        B, T, _ = X.shape\n",
        "\n",
        "        # 1) Project X into packed Q, K, V\n",
        "        # Shapes: (B, T, d_model)\n",
        "        Q = self.Wq(X)\n",
        "        K = self.Wk(X)\n",
        "        V = self.Wv(X)\n",
        "\n",
        "        # 2) Reshape packed (B, T, d_model) -> (B, h, T, d_k)\n",
        "        # - view splits the last dimension into (h, d_k)\n",
        "        # - transpose makes heads dimension come before time\n",
        "        Q = Q.view(B, T, self.h, self.d_k).transpose(1, 2)  # (B, h, T, d_k)\n",
        "        K = K.view(B, T, self.h, self.d_k).transpose(1, 2)  # (B, h, T, d_k)\n",
        "        V = V.view(B, T, self.h, self.d_k).transpose(1, 2)  # (B, h, T, d_k)\n",
        "\n",
        "        # 3) Apply scaled dot-product attention per head (in parallel)\n",
        "        # out:  (B, h, T, d_k)\n",
        "        # attn: (B, h, T, T)\n",
        "        out, attn = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
        "\n",
        "        # 4) Concatenate heads: (B, h, T, d_k) -> (B, T, d_model)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
        "\n",
        "        # 5) Final linear projection back to d_model\n",
        "        out = self.Wo(out)\n",
        "\n",
        "        return out, attn\n"
      ],
      "metadata": {
        "id": "DTTHiInVPFSV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing multihead\n",
        "\n",
        "torch.manual_seed(0)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "B, T, d_model, h = 1, 4, 8, 2\n",
        "X = torch.randn(B, T, d_model, device=device)\n",
        "\n",
        "mha = MultiHeadAttention(d_model=d_model, num_heads=h).to(device)\n",
        "\n",
        "# causal mask: (B, 1, T, T) -> broadcasts to (B, h, T, T)\n",
        "mask = torch.tril(torch.ones(T, T, device=device)).view(1, 1, T, T).expand(B, 1, T, T)\n",
        "\n",
        "out, attn = mha(X, mask=mask)\n",
        "\n",
        "print(\"X:\", X.shape)        # (1, 4, 8)\n",
        "print(\"out:\", out.shape)    # (1, 4, 8)\n",
        "print(\"attn:\", attn.shape)  # (1, 2, 4, 4)\n",
        "\n",
        "print(\"\\nHead 0 attention:\\n\", attn[0,0].detach().cpu())\n",
        "print(\"\\nHead 0 row sums:\\n\", attn[0,0].sum(dim=-1).detach().cpu())\n",
        "\n",
        "print(\"\\nHead 1 attention:\\n\", attn[0,1].detach().cpu())\n",
        "print(\"\\nHead 1 row sums:\\n\", attn[0,1].sum(dim=-1).detach().cpu())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHMwxhbOPKNW",
        "outputId": "961c9a18-b72a-41e4-f991-bc159b01b91d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: torch.Size([1, 4, 8])\n",
            "out: torch.Size([1, 4, 8])\n",
            "attn: torch.Size([1, 2, 4, 4])\n",
            "\n",
            "Head 0 attention:\n",
            " tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5478, 0.4522, 0.0000, 0.0000],\n",
            "        [0.3274, 0.2866, 0.3861, 0.0000],\n",
            "        [0.2485, 0.2721, 0.2112, 0.2682]])\n",
            "\n",
            "Head 0 row sums:\n",
            " tensor([1., 1., 1., 1.])\n",
            "\n",
            "Head 1 attention:\n",
            " tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4438, 0.5562, 0.0000, 0.0000],\n",
            "        [0.3240, 0.3522, 0.3239, 0.0000],\n",
            "        [0.2426, 0.2616, 0.2515, 0.2442]])\n",
            "\n",
            "Head 1 row sums:\n",
            " tensor([1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    }
  ]
}