{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPrhnjMQq+3egSksbN8HKam",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidCastroPena/Vaswani2017/blob/main/replicatingAttentionIsAllWhatYouNeed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnreNdFpCsLR",
        "outputId": "ca056de6-ec51-4eb7-eef6-d85b63ce5c01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# when we are using a transformer to analyze say a sentence (\"the cat sat\"), first what happens is that each word gets stored in a\n",
        "# query: q_sat. Now each query or what i am looking to analize is compared (dot product) with its all the keys; all words have keys: k_the, k_cat,\n",
        "# and k_sat. The dot product geometrically shows that if the direction of two vectors is related, this will reflect a large dot\n",
        "#product, which shows that the two tokens are related\n",
        "\n",
        "\"\"\"\n",
        "qsatâ‹…kthe= 0.1\n",
        "\tâ€‹\n",
        "ð‘žsatâ‹…ð‘˜cat=2.1\n",
        "\n",
        "qsatâ‹…kcat=1.5\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# \"sat\" is more related to cat than the\n",
        "\n",
        "#Recall, the keys and query are linear transformation of their embeddings. Intuitively, we say that the query is a question\n",
        "#per token that aims to uncover the role of the specific word in a given text. The word â€œquestionâ€ is shorthand for\n",
        "#something very precise: The query defines a direction in vector space along which relevance is measured.\n",
        "\n",
        "# The key part is to understand Attention(Q,K,V)=softmax(QK^T/root(d_k)*V\n",
        "\n",
        "#Key concepts:\n",
        "\n",
        "#PyTorch tensors\n",
        "\n",
        "#Matrix multiplication\n",
        "\n",
        "#Softmax\n",
        "\n",
        "#Masking\n",
        "\n",
        "#Shape reasoning\n",
        "\n",
        "\n",
        "#Setup\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(0)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Q,K ,V, and a mask are tensor. A tensor is a multi-dimensional array of numbers and in transformers all the meaning\n",
        "#arises from learned tensor transformations and interactions\n",
        "\n",
        "# embeddings are vectors, but to make computations or to represent sentences, instead of converting to\n",
        "# vectors in a sequential way or one by one, tensors allow you to represent text in a more efficient way\n",
        "\n",
        "\n",
        "# A vector is a tensor (a 1-D tensor). A matrix is a tensor (a 2-D tensor). So the real question is:\n",
        "\n",
        "# Why do we need higher-dimensional tensors instead of just one vector at a time? Imagine you process one word at a time,\n",
        "# using vectors only. Sentence: â€œThe cat satâ€.You would have to do this:\n",
        "\n",
        "# Take embedding of \"The\" â†’ vector compare it to \"cat\" â†’ vector\n",
        "\n",
        "# Compare it to \"sat\" â†’ vector\n",
        "\n",
        "# Repeat for \"cat\"\n",
        "\n",
        "# Repeat for \"sat\"\n",
        "\n",
        "# Thatâ€™s nested loops sequential computation very slow hard to parallelize messy gradients. This is basically how early RNNs worked.\n",
        "\n",
        "# What tensors give you: structure Tensors let you represent many things at once. Instead of:\n",
        "\n",
        "# â€œOne word â†’ one vectorâ€ You represent: All words, all positions, all heads, all batches â€” at the same time\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Q: (batch, heads, q_len, d_k)\n",
        "    K: (batch, heads, k_len, d_k)\n",
        "    V: (batch, heads, k_len, d_v)\n",
        "    mask: (batch, heads, q_len, k_len) with 1 for allowed, 0 for blocked (optional)\n",
        "\n",
        "    returns:\n",
        "      out: (batch, heads, q_len, d_v)\n",
        "      attn: (batch, heads, q_len, k_len)\n",
        "    \"\"\"\n",
        "    d_k = Q.size(-1)\n",
        "\n",
        "    # (batch, heads, q_len, k_len)\n",
        "\n",
        "  #When you compute qverb*kj, you are asking:\n",
        "  # \"does token k, in its current representation, lie in a directaion that is useful for a verb right now?\"\n",
        "  # if the answer is yes (large dot product), attention weight increases, information form that token flows\n",
        "\n",
        "  # K defines how each token can be matched by others.\n",
        "  # It is the â€œinterfaceâ€ a token exposes to incoming queries.\n",
        "  # A learned projection that determines under what kinds of queries this token should become relevant.\n",
        "\n",
        "\n",
        "    scores = Q @ K.transpose(-2, -1) / (d_k ** 0.5)\n",
        "\n",
        "    if mask is not None:\n",
        "        # Set blocked positions to a large negative value so softmax ~ 0 there.\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "#the softmax ensure converting the raw scores into probabilities of how related or important are\n",
        "#the tokens to each other. However, it does not have the information itself: recall that q\n",
        "#contains the question the word asks about its role, the K decide how to be matched, and V decide what\n",
        "# information to contribute.\n",
        "\n",
        "# Consider the word \"bank\". For matching, you may want to match syntactic role or entity type.\n",
        "# For content, you may want to pass financial or river meaning. In this case, Kj encodes:\n",
        "# â€œIf another token is looking for this kind of information, how strongly should I respond?â€ This response\n",
        "#is continuous, context-dependet, and learned statistically\n",
        "\n",
        "# Sentence â€œThe cat sat.â€ During training, the model repeatedly observes that: verbs benefit from attending to certain tokens\n",
        "# those tokens often share embedding patterns (things we humans call â€œnounsâ€)\n",
        "# Over time:\n",
        "\n",
        "  # Wk learns to map those tokens into a region of space\n",
        "  # that region is useful when queried by verbs\n",
        "\n",
        "#  learns to map those tokens into a region of space,that region is useful when queried by verbs\n",
        "\n",
        "# But the model never learns:\n",
        "\n",
        "# â€œThis is a nounâ€\n",
        "\n",
        "# It learns: â€œTokens with embeddings like this tend to be useful when verbs query the context.â€ Thatâ€™s a relational regularity, not a category.\n",
        "\n",
        "\n",
        "    attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "# # the multiplication between the attn and v: The multiplication of attention weights by values computes a learned,\n",
        "# conditional linear combination of representations â€” mathematically similar to a regression or mixture model â€”\n",
        "# but embedded inside a deep, nonlinear system.\n",
        "\n",
        "\n",
        "    out = attn @ V\n",
        "    return out, attn\n",
        "\n"
      ],
      "metadata": {
        "id": "0evCiQo3C6lY"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets create a small example to see the attention at work\n",
        "\n",
        "batch, heads, seq_len, d_k, d_v = 1, 1, 4, 3, 2\n",
        "\n",
        "Q = torch.randn(batch, heads, seq_len, d_k, device=device)\n",
        "K = torch.randn(batch, heads, seq_len, d_k, device=device)\n",
        "V = torch.randn(batch, heads, seq_len, d_v, device=device)\n",
        "\n",
        "out, attn = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "print(\"Q shape:\", Q.shape)\n",
        "print(\"attn shape:\", attn.shape)\n",
        "print(\"out shape:\", out.shape)\n",
        "\n",
        "print(\"\\nAttention matrix (q_len x k_len):\\n\", attn[0,0].detach().cpu())\n",
        "print(\"\\nRow sums (should be ~1):\\n\", attn[0,0].sum(dim=-1).detach().cpu())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDKyBI-mL8VM",
        "outputId": "476aa7d5-1918-4df7-f1d3-99cd33b0e59d"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q shape: torch.Size([1, 1, 4, 3])\n",
            "attn shape: torch.Size([1, 1, 4, 4])\n",
            "out shape: torch.Size([1, 1, 4, 2])\n",
            "\n",
            "Attention matrix (q_len x k_len):\n",
            " tensor([[0.1118, 0.1236, 0.1502, 0.6144],\n",
            "        [0.2187, 0.1804, 0.2324, 0.3685],\n",
            "        [0.1977, 0.1973, 0.2325, 0.3726],\n",
            "        [0.3713, 0.0648, 0.4710, 0.0929]])\n",
            "\n",
            "Row sums (should be ~1):\n",
            " tensor([1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 1ï¸âƒ£ What we implemented first: scaled dot-product attention (pure math)\n",
        "\n",
        "We started by implementing:\n",
        "\n",
        "Attention(Q, K, V) = softmax(QKáµ€ / âˆšdâ‚–) Â· V\n",
        "\n",
        "This function:\n",
        "\n",
        "- takes **Q, K, V as inputs**\n",
        "- computes similarity scores via dot products\n",
        "- normalizes them with softmax\n",
        "- uses them to compute a weighted sum of values\n",
        "\n",
        "At this stage:\n",
        "- Q, K, V were **dummy tensors** (random numbers)\n",
        "- the goal was to understand:\n",
        "  - tensor shapes\n",
        "  - matrix multiplication\n",
        "  - softmax\n",
        "  - attention matrices\n",
        "  - how information flows\n",
        "\n",
        "This step was **intentional** and **necessary**.\n",
        "\n",
        "ðŸ‘‰ Key point:\n",
        "> `scaled_dot_product_attention` is a **standalone mathematical operator**.\n",
        "> It does NOT know where Q, K, V come from.\n",
        "\n",
        "Think of it as the **engine** of attention.\n",
        "\n",
        "---\n",
        "\n",
        "## 2ï¸âƒ£ The missing question: where do Q, K, V come from in a real Transformer?\n",
        "\n",
        "In the actual Transformer paper:\n",
        "\n",
        "- Q, K, V are **not given directly**\n",
        "- They are **computed from token embeddings**\n",
        "\n",
        "The real pipeline is:\n",
        "\n",
        "tokens  \n",
        "â†’ embeddings X  \n",
        "â†’ linear projections  \n",
        "â†’ Q, K, V  \n",
        "â†’ attention\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "Q = X Â· W_Q  \n",
        "K = X Â· W_K  \n",
        "V = X Â· W_V  \n",
        "\n",
        "Where:\n",
        "- X is the embedding tensor (B Ã— T Ã— d_model)\n",
        "- W_Q, W_K, W_V are **learned weight matrices**\n",
        "\n",
        "So Q, K, V are:\n",
        "- different *views* of the same tokens\n",
        "- learned automatically during training\n",
        "\n",
        "---\n",
        "\n",
        "## 3ï¸âƒ£ Why `SelfAttentionHead` exists\n",
        "\n",
        "The purpose of `SelfAttentionHead` is **not to introduce new math**.\n",
        "\n",
        "Its only job is to:\n",
        "\n",
        "1. Take token embeddings `X`\n",
        "2. Create Q, K, V using learned linear layers\n",
        "3. Call `scaled_dot_product_attention(Q, K, V)`\n",
        "4. Return:\n",
        "   - contextualized token representations\n",
        "   - the attention matrix\n",
        "\n",
        "Conceptually:\n",
        "\n",
        "SelfAttentionHead(X)\n",
        "    â”œâ”€â”€ computes Q = XW_Q\n",
        "    â”œâ”€â”€ computes K = XW_K\n",
        "    â”œâ”€â”€ computes V = XW_V\n",
        "    â””â”€â”€ calls scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "So:\n",
        "\n",
        "- `scaled_dot_product_attention` = the **math**\n",
        "- `SelfAttentionHead` = the **first real Transformer component**\n",
        "\n",
        "Nothing magical is happening here.\n",
        "\n",
        "---\n",
        "\n",
        "## 4ï¸âƒ£ Why new tensor dimensions suddenly appear (the â€œheadsâ€ dimension)\n",
        "\n",
        "The attention function is written to support **multi-head attention** later.\n",
        "\n",
        "For that reason, it expects tensors shaped like:\n",
        "\n",
        "(Batch, Heads, SequenceLength, Dimension)\n",
        "\n",
        "Even when:\n",
        "- batch = 1\n",
        "- heads = 1\n",
        "\n",
        "So inside `SelfAttentionHead`, we do:\n",
        "\n",
        "Q = Q.unsqueeze(1)\n",
        "\n",
        "This:\n",
        "- does NOT change the meaning of Q\n",
        "- only adds a placeholder dimension for heads\n",
        "- allows the same function to work for 1 head or many heads\n",
        "\n",
        "ðŸ‘‰ This is a **software design choice**, not a conceptual change.\n",
        "\n",
        "---\n",
        "\n",
        "## 5ï¸âƒ£ Why this step is part of the replication plan\n",
        "\n",
        "This step corresponds to **Section 3.2.1** of the paper:\n",
        "> â€œScaled Dot-Product Attentionâ€\n",
        "\n",
        "But now implemented as:\n",
        "- a **learnable module**\n",
        "- not just a formula on paper\n",
        "\n",
        "At this point in the replication, we are still in:\n",
        "\n",
        "âœ… Step 1 â€” Build attention correctly\n",
        "\n",
        "We are NOT yet:\n",
        "- stacking layers\n",
        "- using multiple heads\n",
        "- training a model\n",
        "- optimizing performance\n",
        "\n",
        "We are simply answering:\n",
        "\n",
        "> â€œHow does a Transformer turn embeddings into attention outputs?â€\n",
        "\n",
        "---\n",
        "\n",
        "## 6ï¸âƒ£ Key takeaway (lock this in)\n",
        "\n",
        "Nothing new was introduced conceptually.\n",
        "\n",
        "We only:\n",
        "- connected the attention math\n",
        "- to learnable parameters\n",
        "- in a reusable module\n",
        "\n",
        "If you understand:\n",
        "- how scaled dot-product attention works\n",
        "- and that `SelfAttentionHead` just *creates Q, K, V*\n",
        "\n",
        "then you understand this part of the Transformer.\n",
        "\n",
        "---\n",
        "\n",
        "## 7ï¸âƒ£ What comes next (but not yet)\n",
        "\n",
        "Only AFTER this is fully clear do we move on to:\n",
        "- masking (causal attention)\n",
        "- multi-head attention\n",
        "- stacking layers\n",
        "\n",
        "We are still building the foundation.\n"
      ],
      "metadata": {
        "id": "ow7_fZ-NCx8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# This class implements a SINGLE self-attention head: he SelfAttentionHead class, which inherits from nn.Module,\n",
        "# represents a concrete component within a neural network. Its primary responsibility is to create the Q, K, and V tensors from the input embeddings X using learnable linear projections (Wq, Wk, Wv).\n",
        "# Why it's a class (inheriting from nn.Module): Classes inheriting from nn.Module in PyTorch are designed for components that:\n",
        "# Contain Learnable Parameters: The Wq, Wk, and Wv matrices are nn.Linear layers, which are learnable. nn.Module manages these parameters, making them discoverable by optimizers during training.\n",
        "# Define a forward pass: The forward method encapsulates the sequence of operations (creating Q/K/V, calling scaled_dot_product_attention, handling dimensions) that this specific module performs.\n",
        "# Are part of a larger graph: By being an nn.Module, it can be easily integrated into a larger neural network (like a full Transformer block), allowing for proper dependency tracking and automatic differentiation.\n",
        "\n",
        "class SelfAttentionHead(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, d_k: int, d_v: int):\n",
        "        super().__init__()\n",
        "\n",
        "# At this point, we are passing from embeddings per each token to the matrices WQ,WK, and V learned matrices.\n",
        "# The learned\n",
        "        # Linear projection that maps embeddings -> query vectors\n",
        "        # Wq is a learned matrix of shape (d_model, d_k)\n",
        "        self.Wq = nn.Linear(d_model, d_k, bias=False)\n",
        "\n",
        "        # Linear projection that maps embeddings -> key vectors\n",
        "        # Wk is a learned matrix of shape (d_model, d_k)\n",
        "        self.Wk = nn.Linear(d_model, d_k, bias=False)\n",
        "\n",
        "        # Linear projection that maps embeddings -> value vectors\n",
        "        # Wv is a learned matrix of shape (d_model, d_v)\n",
        "        self.Wv = nn.Linear(d_model, d_v, bias=False)\n",
        "#X: (B, T, d_model) which is a fundamental way we describe tensors in deep learning, especially when working with sequences\n",
        "# like text. Let's break down each part:X: This is the input tensor itself. In the context of a Transformer,\n",
        "# it usually represents a batch of token embeddings.\n",
        "\n",
        "# B (Batch Size):\n",
        "\n",
        "# This is the number of independent sequences (e.g., sentences, phrases, or documents) that are being processed together.\n",
        "# Modern neural networks, especially when using GPUs, process data in 'batches' rather than one item at a time.\n",
        "#This is much more efficient because it allows for parallel computations.\n",
        "# Example: If B=3, it means you're feeding three different sentences into the model simultaneously.\n",
        "\n",
        "#T (Sequence Length / Number of Tokens):\n",
        "\n",
        "# This is the length of each sequence in the batch. It represents how many tokens (words, sub-words, or characters,\n",
        "# depending on your tokenization) are in each individual input sequence.\n",
        "# Example: If T=10, it means each of your B sentences has 10 tokens.\n",
        "# d_model (Model Dimension / Embedding Dimension):\n",
        "\n",
        "# This is the dimensionality of the vector used to represent each single token. Every token (like a word) is\n",
        "#converted into a numerical vector of this size. This vector is meant to capture the semantic and positional\n",
        "#information of that token. Example: If d_model=512, then each token in your T length sequences is represented\n",
        "#by a vector of 512 numbers.\n",
        "\n",
        "    def forward(self, X, mask=None):\n",
        "        \"\"\"\n",
        "        X: (B, T, d_model)\n",
        "        returns:\n",
        "          out:  (B, T, d_v)\n",
        "          attn: (B, T, T)\n",
        "        \"\"\"\n",
        "\n",
        "        # Project embeddings -> Q/K/V\n",
        "        Q = self.Wq(X)  # (B, T, d_k)\n",
        "        K = self.Wk(X)  # (B, T, d_k)\n",
        "        V = self.Wv(X)  # (B, T, d_v)\n",
        "\n",
        "        # Add head dimension so we can reuse the same attention function\n",
        "        Q = Q.unsqueeze(1)  # (B, 1, T, d_k)\n",
        "        K = K.unsqueeze(1)  # (B, 1, T, d_k)\n",
        "        V = V.unsqueeze(1)  # (B, 1, T, d_v)\n",
        "\n",
        "        out, attn = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
        "\n",
        "        # Remove head dimension (since H=1 here)\n",
        "        return out.squeeze(1), attn.squeeze(1)"
      ],
      "metadata": {
        "id": "Qlo4JRLxW6ps"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explaining the  make causal mask: attn[i, j] = how much token i (the query) attends to token j (the key/value).\n",
        "\n",
        "# Because you used a causal mask, token i is only allowed to attend to tokens â‰¤ i (no looking ahead).\n",
        "\n",
        "# Thatâ€™s why everything above the diagonal is zero.\n",
        "\n",
        "def make_causal_mask(B, H, T, device):\n",
        "    # lower-triangular matrix: 1 = allowed, 0 = blocked\n",
        "    m = torch.tril(torch.ones(T, T, device=device))\n",
        "    # expand to (B, H, T, T)\n",
        "    return m.view(1, 1, T, T).expand(B, H, T, T)\n"
      ],
      "metadata": {
        "id": "crV8l9UwEmLG"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Testing the causal masking: torch.manual_seed(0)\n",
        "# Interpreting results:\n",
        "# [0.5445, 0.4555, 0.0000, 0.0000]\n",
        "# ~54% to token 0\n",
        "\n",
        "# Exactly expected.\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "B, T, d_model, d_k, d_v = 1, 4, 8, 4, 4\n",
        "X = torch.randn(B, T, d_model, device=device)\n",
        "\n",
        "head = SelfAttentionHead(d_model, d_k, d_v).to(device)\n",
        "\n",
        "mask = make_causal_mask(B, 1, T, device)  # (B, H=1, T, T)\n",
        "\n",
        "out, attn = head(X, mask=mask)\n",
        "\n",
        "print(\"attn:\\n\", attn[0].detach().cpu())\n",
        "print(\"row sums:\\n\", attn[0].sum(dim=-1).detach().cpu())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LepUvHp5LepZ",
        "outputId": "bd700481-d373-4768-d0fc-a1761fb763cb"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attn:\n",
            " tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5098, 0.4902, 0.0000, 0.0000],\n",
            "        [0.3179, 0.3470, 0.3351, 0.0000],\n",
            "        [0.2265, 0.2280, 0.2319, 0.3136]])\n",
            "row sums:\n",
            " tensor([1., 1., 1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d79de55",
        "outputId": "3896d6de-b284-4db2-9bfa-6d0644f2a60a"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Define dimensions for the SelfAttentionHead\n",
        "d_model = 64  # Embedding dimension\n",
        "d_k = 32      # Dimension of Query and Key vectors\n",
        "d_v = 32      # Dimension of Value vectors\n",
        "\n",
        "# Instantiate the SelfAttentionHead\n",
        "attention_head = SelfAttentionHead(d_model, d_k, d_v).to(device)\n",
        "\n",
        "# Create a dummy input tensor (batch_size, sequence_length, d_model)\n",
        "batch_size = 2\n",
        "sequence_length = 5\n",
        "X = torch.randn(batch_size, sequence_length, d_model, device=device)\n",
        "\n",
        "# Perform a forward pass\n",
        "out, attn = attention_head(X)\n",
        "\n",
        "print(\"Input X shape:\", X.shape)\n",
        "print(\"Output 'out' shape:\", out.shape)\n",
        "print(\"Attention 'attn' shape:\", attn.shape)\n",
        "\n",
        "# Verify row sums of attention matrix (should be ~1)\n",
        "print(\"\\nAttention matrix row sums (first batch, first head, should be ~1):\\n\", attn[0].sum(dim=-1).detach().cpu())"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input X shape: torch.Size([2, 5, 64])\n",
            "Output 'out' shape: torch.Size([2, 5, 32])\n",
            "Attention 'attn' shape: torch.Size([2, 5, 5])\n",
            "\n",
            "Attention matrix row sums (first batch, first head, should be ~1):\n",
            " tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“Œ NOTE â€” The correct mental model: architecture vs learning in the Transformer\n",
        "\n",
        "At this point, it is important to separate **what is being built** from **what is being learned**.\n",
        "\n",
        "The Transformer paper combines mathematics, architecture, and training in one narrative.\n",
        "In this notebook, we are intentionally **separating them**, which is why it may feel like\n",
        "â€œnothing is being learned yetâ€.\n",
        "\n",
        "This note clarifies the correct mental model.\n",
        "\n",
        "---\n",
        "\n",
        "## 1ï¸âƒ£ Three distinct layers of understanding (do not mix them)\n",
        "\n",
        "When reading *Attention Is All You Need*, there are **three different layers** happening at once:\n",
        "\n",
        "---\n",
        "\n",
        "### Layer 1 â€” The mathematical operator (pure attention math)\n",
        "\n",
        "This is the formula:\n",
        "\n",
        "Attention(Q, K, V) = softmax(QKáµ€ / âˆšdâ‚–) Â· V\n",
        "\n",
        "At this layer:\n",
        "- Q, K, V are just tensors\n",
        "- no corpus is required\n",
        "- no learning is required\n",
        "- this is pure linear algebra\n",
        "\n",
        "This is what we implemented first with:\n",
        "- dummy Q, K, V\n",
        "- random tensors\n",
        "- shape checks\n",
        "- attention matrices\n",
        "\n",
        "ðŸ‘‰ Purpose: understand **how attention works mechanically**\n",
        "\n",
        "---\n",
        "\n",
        "### Layer 2 â€” The architectural module (parameterized attention)\n",
        "\n",
        "This is where `SelfAttentionHead` appears.\n",
        "\n",
        "At this layer:\n",
        "- we introduce **learnable matrices** W_Q, W_K, W_V\n",
        "- Q, K, V are computed as:\n",
        "  \n",
        "  Q = X Â· W_Q  \n",
        "  K = X Â· W_K  \n",
        "  V = X Â· W_V  \n",
        "\n",
        "- X are token embeddings\n",
        "- W_Q, W_K, W_V are **trainable parameters**\n",
        "\n",
        "Important:\n",
        "- these matrices are **randomly initialized**\n",
        "- they are *capable* of learning\n",
        "- but they have **not learned anything yet**\n",
        "\n",
        "ðŸ‘‰ Purpose: build the **Transformer architecture**, not train it\n",
        "\n",
        "This corresponds to the *architecture description* in the paper (Section 3).\n",
        "\n",
        "---\n",
        "\n",
        "### Layer 3 â€” Training (where learning actually happens)\n",
        "\n",
        "Learning only happens when we introduce:\n",
        "\n",
        "- a dataset (toy task or real text corpus)\n",
        "- a prediction objective (e.g., next-token prediction)\n",
        "- a loss function\n",
        "- backpropagation\n",
        "- an optimizer\n",
        "\n",
        "Only then do:\n",
        "- W_Q, W_K, W_V\n",
        "- embeddings\n",
        "- all other parameters\n",
        "\n",
        "begin to change in response to data.\n",
        "\n",
        "ðŸ‘‰ Purpose: make attention **meaningful**\n",
        "\n",
        "This corresponds to the *training procedure* sections of the paper.\n",
        "\n",
        "---\n",
        "\n",
        "## 2ï¸âƒ£ What we are doing right now in this notebook\n",
        "\n",
        "We are currently **between Layer 1 and Layer 2**.\n",
        "\n",
        "Specifically:\n",
        "- we fully understand the attention math (Layer 1)\n",
        "- we have built the parameterized attention module (Layer 2)\n",
        "- we have NOT introduced data, loss, or optimization (Layer 3)\n",
        "\n",
        "So it is **expected and correct** that:\n",
        "- W_Q, W_K, W_V exist\n",
        "- but they do not encode syntax, semantics, or grammar\n",
        "- attention patterns look random\n",
        "\n",
        "This is not a bug.\n",
        "This is the correct state at this stage.\n",
        "\n",
        "---\n",
        "\n",
        "## 3ï¸âƒ£ Why we are not using a corpus yet\n",
        "\n",
        "We intentionally do NOT start with real text because:\n",
        "\n",
        "- bugs in attention math are hard to debug once training is added\n",
        "- shape errors become opaque\n",
        "- attention feels like â€œmagicâ€ instead of computation\n",
        "\n",
        "The correct replication order is:\n",
        "\n",
        "1. Build attention\n",
        "2. Verify attention\n",
        "3. Build the Transformer block\n",
        "4. Train on **toy tasks**\n",
        "5. Only then train on real text\n",
        "\n",
        "This is exactly the plan we are following.\n",
        "\n",
        "---\n",
        "\n",
        "## 4ï¸âƒ£ How the paperâ€™s wording can be misleading\n",
        "\n",
        "When the paper says:\n",
        "> â€œWe use learned linear projections to obtain the queries, keys, and valuesâ€¦â€\n",
        "\n",
        "They are describing the **final trained system**, not the initial state of the code.\n",
        "\n",
        "In code:\n",
        "- those projections exist immediately\n",
        "- but they only become â€œlearnedâ€ after training\n",
        "\n",
        "---\n",
        "\n",
        "## 5ï¸âƒ£ Key takeaway (lock this in)\n",
        "\n",
        "> **Right now, we are building the machine.  \n",
        "> Learning only begins once we add a dataset, a loss, and optimization.**\n",
        "\n",
        "If you understand:\n",
        "- how attention works mathematically\n",
        "- how Q/K/V are produced architecturally\n",
        "- and why learning has not started yet\n",
        "\n",
        "then you have the correct mental model.\n",
        "\n",
        "---\n",
        "\n",
        "## 6ï¸âƒ£ What comes next\n",
        "\n",
        "Before touching real text, we will:\n",
        "- finish attention (masking)\n",
        "- add multi-head attention\n",
        "- stack layers\n",
        "- then train on simple toy tasks\n",
        "\n",
        "Only after that will we introduce a real corpus.\n",
        "\n",
        "This is the correct and disciplined way to replicate the paper.\n"
      ],
      "metadata": {
        "id": "EQA0qm8BDwhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Attention (MHA)\n",
        "\n",
        "    Input:\n",
        "      X: (B, T, d_model)\n",
        "\n",
        "    Output:\n",
        "      out:  (B, T, d_model)\n",
        "      attn: (B, h, T, T)   # attention matrix per head\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.h = num_heads\n",
        "        self.d_k = d_model // num_heads  # dimension per head (queries/keys/values per head)\n",
        "\n",
        "        # Big projections: X -> Q,K,V all in d_model space\n",
        "        # Each contains all heads packed together.\n",
        "        self.Wq = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.Wk = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.Wv = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        # Final output projection: concat(heads) -> d_model\n",
        "        self.Wo = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    def forward(self, X, mask=None):\n",
        "        \"\"\"\n",
        "        X: (B, T, d_model)\n",
        "\n",
        "        mask: should be broadcastable to (B, h, T, T)\n",
        "              - for causal masking you can build (B, 1, T, T) and it will broadcast across heads\n",
        "        \"\"\"\n",
        "        B, T, _ = X.shape\n",
        "\n",
        "        # 1) Project X into packed Q, K, V\n",
        "        # Shapes: (B, T, d_model)\n",
        "        Q = self.Wq(X)\n",
        "        K = self.Wk(X)\n",
        "        V = self.Wv(X)\n",
        "\n",
        "        # 2) Reshape packed (B, T, d_model) -> (B, h, T, d_k)\n",
        "        # - view splits the last dimension into (h, d_k)\n",
        "        # - transpose makes heads dimension come before time\n",
        "        Q = Q.view(B, T, self.h, self.d_k).transpose(1, 2)  # (B, h, T, d_k)\n",
        "        K = K.view(B, T, self.h, self.d_k).transpose(1, 2)  # (B, h, T, d_k)\n",
        "        V = V.view(B, T, self.h, self.d_k).transpose(1, 2)  # (B, h, T, d_k)\n",
        "\n",
        "        # 3) Apply scaled dot-product attention per head (in parallel)\n",
        "        # out:  (B, h, T, d_k)\n",
        "        # attn: (B, h, T, T)\n",
        "        out, attn = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
        "\n",
        "        # 4) Concatenate heads: (B, h, T, d_k) -> (B, T, d_model)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
        "\n",
        "        # 5) Final linear projection back to d_model\n",
        "        out = self.Wo(out)\n",
        "\n",
        "        return out, attn\n"
      ],
      "metadata": {
        "id": "DTTHiInVPFSV"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing multihead\n",
        "\n",
        "torch.manual_seed(0)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "B, T, d_model, h = 1, 4, 8, 2\n",
        "X = torch.randn(B, T, d_model, device=device)\n",
        "\n",
        "mha = MultiHeadAttention(d_model=d_model, num_heads=h).to(device)\n",
        "\n",
        "# causal mask: (B, 1, T, T) -> broadcasts to (B, h, T, T)\n",
        "mask = torch.tril(torch.ones(T, T, device=device)).view(1, 1, T, T).expand(B, 1, T, T)\n",
        "\n",
        "out, attn = mha(X, mask=mask)\n",
        "\n",
        "print(\"X:\", X.shape)        # (1, 4, 8)\n",
        "print(\"out:\", out.shape)    # (1, 4, 8)\n",
        "print(\"attn:\", attn.shape)  # (1, 2, 4, 4)\n",
        "\n",
        "print(\"\\nHead 0 attention:\\n\", attn[0,0].detach().cpu())\n",
        "print(\"\\nHead 0 row sums:\\n\", attn[0,0].sum(dim=-1).detach().cpu())\n",
        "\n",
        "print(\"\\nHead 1 attention:\\n\", attn[0,1].detach().cpu())\n",
        "print(\"\\nHead 1 row sums:\\n\", attn[0,1].sum(dim=-1).detach().cpu())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHMwxhbOPKNW",
        "outputId": "a5cdce12-3638-4063-d1cf-b1d19815fd95"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: torch.Size([1, 4, 8])\n",
            "out: torch.Size([1, 4, 8])\n",
            "attn: torch.Size([1, 2, 4, 4])\n",
            "\n",
            "Head 0 attention:\n",
            " tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4702, 0.5298, 0.0000, 0.0000],\n",
            "        [0.2520, 0.4920, 0.2559, 0.0000],\n",
            "        [0.2779, 0.4225, 0.1195, 0.1802]])\n",
            "\n",
            "Head 0 row sums:\n",
            " tensor([1.0000, 1.0000, 1.0000, 1.0000])\n",
            "\n",
            "Head 1 attention:\n",
            " tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5070, 0.4930, 0.0000, 0.0000],\n",
            "        [0.1008, 0.3130, 0.5862, 0.0000],\n",
            "        [0.1322, 0.2023, 0.2607, 0.4049]])\n",
            "\n",
            "Head 1 row sums:\n",
            " tensor([1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFFN(nn.Module):\n",
        "    # FFN(x) = Linear(d_model -> d_ff) + ReLU + Linear(d_ff -> d_model)\n",
        "    def __init__(self, d_model: int, d_ff: int):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(d_model, d_ff)\n",
        "        self.lin2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin2(F.relu(self.lin1(x)))\n",
        "\n",
        "\n",
        "class TransformerBlockPostNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    One Transformer decoder block (post-norm like the original paper):\n",
        "\n",
        "      a = MHA(x)\n",
        "      x = LayerNorm(x + dropout(a))\n",
        "      f = FFN(x)\n",
        "      x = LayerNorm(x + dropout(f))\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.ffn = PositionwiseFFN(d_model=d_model, d_ff=d_ff)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention sublayer + residual + norm\n",
        "        attn_out, attn = self.mha(x, mask=mask)\n",
        "        x = self.norm1(x + self.drop(attn_out))\n",
        "\n",
        "        # FFN sublayer + residual + norm\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.norm2(x + self.drop(ffn_out))\n",
        "\n",
        "        return x, attn\n"
      ],
      "metadata": {
        "id": "ENrERw7cryHo"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "B, T, d_model, h, d_ff = 1, 4, 8, 2, 32\n",
        "x = torch.randn(B, T, d_model, device=device)\n",
        "\n",
        "block = TransformerBlockPostNorm(d_model=d_model, num_heads=h, d_ff=d_ff, dropout=0.0).to(device)\n",
        "\n",
        "mask = torch.tril(torch.ones(T, T, device=device)).view(1, 1, T, T).expand(B, 1, T, T)\n",
        "\n",
        "y, attn = block(x, mask=mask)\n",
        "\n",
        "print(\"x:\", x.shape)        # (1,4,8)\n",
        "print(\"y:\", y.shape)        # (1,4,8)\n",
        "print(\"attn:\", attn.shape)  # (1,2,4,4)\n",
        "print(\"head0 row sums:\", attn[0,0].sum(dim=-1).detach().cpu())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CBW6cGOtULq",
        "outputId": "1e31cc82-a721-45ad-c98a-1ecd3bd4f66e"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: torch.Size([1, 4, 8])\n",
            "y: torch.Size([1, 4, 8])\n",
            "attn: torch.Size([1, 2, 4, 4])\n",
            "head0 row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Encoding\n",
        "\n",
        "import math\n",
        "\n",
        "def sinusoidal_positional_encoding(T: int, d_model: int, device):\n",
        "    \"\"\"\n",
        "    Returns PE of shape (1, T, d_model) so it can be added to X: (B, T, d_model)\n",
        "    \"\"\"\n",
        "    pe = torch.zeros(T, d_model, device=device)\n",
        "\n",
        "    # positions: (T, 1) = [[0],[1],[2],...]\n",
        "    position = torch.arange(0, T, device=device).unsqueeze(1).float()\n",
        "\n",
        "    # div_term: (d_model/2,) controlling frequencies\n",
        "    div_term = torch.exp(\n",
        "        torch.arange(0, d_model, 2, device=device).float() * (-math.log(10000.0) / d_model)\n",
        "    )\n",
        "\n",
        "    # Apply sin to even indices, cos to odd indices\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)  # even dims\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)  # odd dims\n",
        "\n",
        "    return pe.unsqueeze(0)  # (1, T, d_model)\n"
      ],
      "metadata": {
        "id": "-2Borv6TuInU"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test positional encoding\n",
        "\n",
        "T, d_model = 4, 8\n",
        "pe = sinusoidal_positional_encoding(T, d_model, device=device)\n",
        "\n",
        "print(\"PE shape:\", pe.shape)  # (1,4,8)\n",
        "print(\"PE[0,0]:\", pe[0,0])    # position 0\n",
        "print(\"PE[0,1]:\", pe[0,1])    # position 1 (should differ)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgVDSix7uPFE",
        "outputId": "44127d2e-c2d9-4156-a9e9-9e7e4c94a878"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PE shape: torch.Size([1, 4, 8])\n",
            "PE[0,0]: tensor([0., 1., 0., 1., 0., 1., 0., 1.], device='cuda:0')\n",
            "PE[0,1]: tensor([0.8415, 0.5403, 0.0998, 0.9950, 0.0100, 0.9999, 0.0010, 1.0000],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Synthetic batch generator\n",
        "\n",
        "def make_copy_batch_simple(batch_size, seq_len, vocab_size, device):\n",
        "    x = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
        "    return x, x\n"
      ],
      "metadata": {
        "id": "R9PUGTVzuZLp"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimal decoder-only transformer\n",
        "\n",
        "class TinyDecoderLM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # token embedding\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # stack of Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlockPostNorm(d_model=d_model, num_heads=num_heads, d_ff=d_ff, dropout=dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # final projection to vocabulary logits\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        \"\"\"\n",
        "        token_ids: (B, T)\n",
        "        returns:\n",
        "          logits: (B, T, vocab_size)\n",
        "          attn_last: (B, h, T, T) attention from the last block (for inspection)\n",
        "        \"\"\"\n",
        "        B, T = token_ids.shape\n",
        "        device = token_ids.device\n",
        "\n",
        "        # embeddings: (B, T, d_model)\n",
        "        x = self.tok_emb(token_ids)\n",
        "\n",
        "        # positional encoding: (1, T, d_model) then broadcast\n",
        "        pe = sinusoidal_positional_encoding(T, self.d_model, device=device)\n",
        "        x = x + pe\n",
        "\n",
        "        # causal mask: allow attending to self and past only\n",
        "        # shape (B, 1, T, T) broadcasts across heads\n",
        "        mask = torch.tril(torch.ones(T, T, device=device)).view(1, 1, T, T).expand(B, 1, T, T)\n",
        "\n",
        "        attn_last = None\n",
        "        for block in self.blocks:\n",
        "            x, attn_last = block(x, mask=mask)\n",
        "\n",
        "        # logits: (B, T, vocab_size)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits, attn_last\n",
        "\n"
      ],
      "metadata": {
        "id": "DBa2GsAiwxB6"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "\n",
        "# --- Hyperparams (small, runs fast) ---\n",
        "vocab_size = 20\n",
        "seq_len = 16\n",
        "batch_size = 64\n",
        "\n",
        "d_model = 64\n",
        "num_heads = 4\n",
        "d_ff = 256\n",
        "num_layers = 2\n",
        "dropout = 0.0\n",
        "\n",
        "steps = 600\n",
        "lr = 1e-3\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)\n",
        "\n",
        "model = TinyDecoderLM(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    d_ff=d_ff,\n",
        "    num_layers=num_layers,\n",
        "    max_len=seq_len,\n",
        "    dropout=dropout\n",
        ").to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "for step in range(1, steps + 1):\n",
        "    inp, tgt = make_copy_batch_simple(batch_size, seq_len, vocab_size, device=device)\n",
        "\n",
        "    logits, _ = model(inp)  # (B,T,V)\n",
        "\n",
        "    # CrossEntropyLoss expects (N, C) and target (N,)\n",
        "    loss = loss_fn(logits.reshape(-1, vocab_size), tgt.reshape(-1))\n",
        "\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(f\"step {step:4d} | loss {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33fzssbWxGki",
        "outputId": "39b56d16-4c0f-4689-e740-6c8201d3c12d"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n",
            "step  100 | loss 0.0249\n",
            "step  200 | loss 0.0093\n",
            "step  300 | loss 0.0049\n",
            "step  400 | loss 0.0031\n",
            "step  500 | loss 0.0021\n",
            "step  600 | loss 0.0016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick Evaluation\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    inp, tgt = make_copy_batch_simple(1, seq_len, vocab_size, device=device)\n",
        "    logits, attn = model(inp)\n",
        "    pred = torch.argmax(logits, dim=-1)\n",
        "\n",
        "print(\"inp:\", inp[0].tolist())\n",
        "print(\"tgt:\", tgt[0].tolist())\n",
        "print(\"pred:\", pred[0].tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlOa7XDBxK-l",
        "outputId": "a8749267-62a5-461e-8944-112701efac8b"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inp: [1, 12, 19, 14, 12, 0, 1, 11, 0, 10, 1, 6, 19, 18, 4, 18]\n",
            "tgt: [1, 12, 19, 14, 12, 0, 1, 11, 0, 10, 1, 6, 19, 18, 4, 18]\n",
            "pred: [1, 12, 19, 14, 12, 0, 1, 11, 0, 10, 1, 6, 19, 18, 4, 18]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect one head attention from the last layer\n",
        "\n",
        "print(\"attn shape:\", attn.shape)  # (B, h, T, T)\n",
        "print(\"head0 attn:\\n\", attn[0,0].detach().cpu())\n",
        "print(\"head0 row sums:\\n\", attn[0,0].sum(dim=-1).detach().cpu())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRglJmVNxPtS",
        "outputId": "48235cda-402d-423e-b454-d13fd7688d13"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attn shape: torch.Size([1, 4, 16, 16])\n",
            "head0 attn:\n",
            " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2543, 0.7457, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1946, 0.2332, 0.5722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1617, 0.3091, 0.2362, 0.2930, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1206, 0.2955, 0.1115, 0.1883, 0.2840, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1662, 0.0853, 0.2552, 0.0730, 0.0957, 0.3247, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1221, 0.1248, 0.0574, 0.1535, 0.1275, 0.2566, 0.1579, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0639, 0.0942, 0.0287, 0.1093, 0.0945, 0.1155, 0.0810, 0.4129, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1221, 0.0589, 0.1661, 0.0531, 0.0648, 0.2102, 0.1201, 0.0426, 0.1622,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0821, 0.0881, 0.0600, 0.0983, 0.0903, 0.0540, 0.0832, 0.0462, 0.0605,\n",
            "         0.3373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0672, 0.0628, 0.0291, 0.0749, 0.0630, 0.1212, 0.0837, 0.2817, 0.1187,\n",
            "         0.0298, 0.0678, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0712, 0.0729, 0.0646, 0.0914, 0.0719, 0.0904, 0.0747, 0.1604, 0.0909,\n",
            "         0.0477, 0.0725, 0.0914, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0650, 0.0739, 0.1709, 0.0500, 0.0794, 0.0851, 0.0700, 0.0351, 0.0698,\n",
            "         0.0208, 0.0641, 0.0451, 0.1708, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0591, 0.1032, 0.0341, 0.0786, 0.0998, 0.0653, 0.0638, 0.1573, 0.0612,\n",
            "         0.0409, 0.0568, 0.0585, 0.0322, 0.0891, 0.0000, 0.0000],\n",
            "        [0.0756, 0.0786, 0.0532, 0.0798, 0.0703, 0.0406, 0.0658, 0.0640, 0.0461,\n",
            "         0.0890, 0.0740, 0.0655, 0.0497, 0.0888, 0.0588, 0.0000],\n",
            "        [0.0527, 0.0889, 0.0305, 0.0670, 0.0854, 0.0518, 0.0559, 0.1280, 0.0497,\n",
            "         0.0371, 0.0507, 0.0519, 0.0290, 0.0891, 0.0417, 0.0905]])\n",
            "head0 row sums:\n",
            " tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets check and confirm learning accuracy\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    inp, tgt = make_copy_batch_simple(64, seq_len, vocab_size, device=device)\n",
        "    logits, attn = model(inp)\n",
        "    pred = logits.argmax(dim=-1)\n",
        "    acc = (pred == tgt).float().mean().item()\n",
        "\n",
        "print(\"token accuracy:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yd6R1dtx6hv",
        "outputId": "ae114401-fcda-4336-9c13-0929217eb14b"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse task (strong long-range diagnostic)\n",
        "# Input: [a, b, c, d, ...]\n",
        "# Target: [... d, c, b, a]\n",
        "\n",
        "def make_reverse_batch(batch_size, seq_len, vocab_size, device):\n",
        "    x = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
        "    y = torch.flip(x, dims=[1])  # reverse along sequence dimension\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "Ne8N8gh3zogd"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on reverse\n",
        "torch.manual_seed(0)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "vocab_size = 20\n",
        "seq_len = 32        # increase length to stress long-range\n",
        "batch_size = 128\n",
        "\n",
        "d_model = 64\n",
        "num_heads = 4\n",
        "d_ff = 128\n",
        "num_layers = 2\n",
        "dropout = 0.0\n",
        "\n",
        "steps = 1200\n",
        "lr = 1e-3\n",
        "\n",
        "model = TinyDecoderLM(vocab_size, d_model, num_heads, d_ff, num_layers, max_len=seq_len, dropout=dropout).to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "for step in range(1, steps + 1):\n",
        "    inp, tgt = make_reverse_batch(batch_size, seq_len, vocab_size, device=device)\n",
        "    logits, _ = model(inp)\n",
        "    loss = loss_fn(logits.reshape(-1, vocab_size), tgt.reshape(-1))\n",
        "\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    if step % 200 == 0:\n",
        "        pred = logits.argmax(dim=-1)\n",
        "        acc = (pred == tgt).float().mean().item()\n",
        "        print(f\"step {step:4d} | loss {loss.item():.4f} | acc {acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eujOOQ-00TxT",
        "outputId": "400e1c39-3b2b-47cf-8c5c-d00b2c74ecd6"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step  200 | loss 1.6864 | acc 0.469\n",
            "step  400 | loss 1.5021 | acc 0.522\n",
            "step  600 | loss 1.5012 | acc 0.525\n",
            "step  800 | loss 1.4985 | acc 0.526\n",
            "step 1000 | loss 1.5000 | acc 0.524\n",
            "step 1200 | loss 1.5002 | acc 0.526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate reverse accuracy\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    inp, tgt = make_reverse_batch(64, seq_len, vocab_size, device=device)\n",
        "    logits, attn = model(inp)\n",
        "    pred = logits.argmax(dim=-1)\n",
        "    acc = (pred == tgt).float().mean().item()\n",
        "\n",
        "print(\"reverse token accuracy:\", acc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqylNNcG0cVs",
        "outputId": "3293f79d-4dd9-43cc-9e2f-6023aff3c582"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reverse token accuracy: 0.52587890625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect attention (all heads, last layer)\n",
        "print(\"attn shape:\", attn.shape)  # (B, h, T, T)\n",
        "\n",
        "for h in range(attn.shape[1]):\n",
        "    print(f\"\\n--- head {h} ---\")\n",
        "    print(attn[0, h].detach().cpu())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLVvXQOt0j1r",
        "outputId": "3f9a79b9-798d-4844-bf20-4d1efd886344"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attn shape: torch.Size([64, 4, 32, 32])\n",
            "\n",
            "--- head 0 ---\n",
            "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [9.4505e-01, 5.4946e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [4.7913e-01, 4.4944e-01, 7.1430e-02,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        ...,\n",
            "        [1.1300e-05, 9.0552e-03, 9.7353e-01,  ..., 1.2562e-18, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [7.6057e-02, 9.0577e-01, 1.8176e-02,  ..., 2.2223e-16, 7.9260e-16,\n",
            "         0.0000e+00],\n",
            "        [8.6793e-01, 1.3205e-01, 2.0111e-06,  ..., 5.3885e-14, 2.6037e-11,\n",
            "         5.9605e-09]])\n",
            "\n",
            "--- head 1 ---\n",
            "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [7.6620e-01, 2.3380e-01, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [6.9845e-02, 5.9142e-01, 3.3874e-01,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        ...,\n",
            "        [1.7123e-06, 3.5058e-03, 9.8521e-01,  ..., 3.8407e-20, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [5.1059e-02, 9.2375e-01, 2.5187e-02,  ..., 2.1072e-16, 3.2249e-16,\n",
            "         0.0000e+00],\n",
            "        [8.3730e-01, 1.6260e-01, 2.2779e-06,  ..., 9.9103e-13, 1.8233e-10,\n",
            "         1.8592e-08]])\n",
            "\n",
            "--- head 2 ---\n",
            "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [9.8263e-01, 1.7367e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [7.3506e-01, 2.4863e-01, 1.6312e-02,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        ...,\n",
            "        [5.0791e-06, 6.9286e-03, 9.8308e-01,  ..., 2.3073e-20, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [7.5628e-02, 9.1086e-01, 1.3515e-02,  ..., 4.2687e-17, 2.2640e-16,\n",
            "         0.0000e+00],\n",
            "        [8.9419e-01, 1.0570e-01, 8.8521e-07,  ..., 1.0950e-13, 7.2597e-11,\n",
            "         7.9603e-09]])\n",
            "\n",
            "--- head 3 ---\n",
            "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [6.5865e-01, 3.4135e-01, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [5.0838e-01, 3.0060e-01, 1.9102e-01,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        ...,\n",
            "        [2.7663e-06, 1.4918e-05, 3.9406e-04,  ..., 2.4996e-04, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [4.6278e-05, 1.3076e-04, 4.2369e-04,  ..., 1.1592e-03, 9.9862e-05,\n",
            "         0.0000e+00],\n",
            "        [8.0530e-04, 1.2167e-03, 3.9287e-04,  ..., 3.1546e-03, 2.2164e-03,\n",
            "         6.0540e-04]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why accuracy stalls around ~0.5\n",
        "# Core issue\n",
        "\n",
        "Youâ€™re using a decoder-only causal Transformer to do sequence-to-sequence reversal.\n",
        "\n",
        "But reversal is fundamentally â€œlook at the whole input, then emit outputâ€. A decoder-only model with a causal mask is forced to produce output at position t using only tokens â‰¤ t of the input â€” it cannot see the future tokens that it would need to perfectly reverse.\n",
        "\n",
        "Thatâ€™s why it canâ€™t reach 1.0 accuracy in this setup.\n",
        "\n",
        "What your heads show\n",
        "\n",
        "Heads 0â€“2 are very peaked early (strongly attend to the beginning of the prefix), and later rows often spike around a few early positions (you see huge mass on column ~2 in late rows). Thatâ€™s consistent with a decoder trying to â€œanchorâ€ to early context because it canâ€™t access the full sequence.\n",
        "\n",
        "Head 3 looks diffuse and tinyâ€”likely a â€œbackgroundâ€ head not carrying the main routing.\n",
        "\n",
        "So: your model is doing the best it can under a causal constraint that makes perfect reversal impossible."
      ],
      "metadata": {
        "id": "OT4AgNWW1Ns3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets dadd a multi head that can do cross attention\n",
        "\n",
        "class MultiHeadAttentionGeneral(nn.Module):\n",
        "    \"\"\"\n",
        "    Works for:\n",
        "      - self-attention: Q_in = K_in = V_in = X\n",
        "      - cross-attention: Q_in = decoder states, K_in/V_in = encoder memory\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.h = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.Wq = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.Wk = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.Wv = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.Wo = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, Q_in, K_in, V_in, mask=None):\n",
        "        \"\"\"\n",
        "        Q_in: (B, Tq, d_model)\n",
        "        K_in: (B, Tk, d_model)\n",
        "        V_in: (B, Tk, d_model)\n",
        "        mask: broadcastable to (B, h, Tq, Tk)\n",
        "        \"\"\"\n",
        "        B, Tq, _ = Q_in.shape\n",
        "        Tk = K_in.shape[1]\n",
        "\n",
        "        Q = self.Wq(Q_in)  # (B,Tq,d_model)\n",
        "        K = self.Wk(K_in)  # (B,Tk,d_model)\n",
        "        V = self.Wv(V_in)  # (B,Tk,d_model)\n",
        "\n",
        "        Q = Q.view(B, Tq, self.h, self.d_k).transpose(1, 2)  # (B,h,Tq,d_k)\n",
        "        K = K.view(B, Tk, self.h, self.d_k).transpose(1, 2)  # (B,h,Tk,d_k)\n",
        "        V = V.view(B, Tk, self.h, self.d_k).transpose(1, 2)  # (B,h,Tk,d_k)\n",
        "\n",
        "        out, attn = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
        "        out = self.drop(out)\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, Tq, self.d_model)  # (B,Tq,d_model)\n",
        "        out = self.Wo(out)\n",
        "        return out, attn\n"
      ],
      "metadata": {
        "id": "8XMkYnO-1XuE"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build encoder-decorder\n",
        "\n",
        "class EncoderBlockPostNorm(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttentionGeneral(d_model, num_heads, dropout=dropout)\n",
        "        self.ffn = PositionwiseFFN(d_model, d_ff)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # full self-attn (no causal mask)\n",
        "        a, _ = self.self_attn(x, x, x, mask=None)\n",
        "        x = self.norm1(x + self.drop(a))\n",
        "\n",
        "        f = self.ffn(x)\n",
        "        x = self.norm2(x + self.drop(f))\n",
        "        return x\n",
        "\n",
        "\n",
        "class DecoderBlockPostNorm(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttentionGeneral(d_model, num_heads, dropout=dropout)\n",
        "        self.cross_attn = MultiHeadAttentionGeneral(d_model, num_heads, dropout=dropout)\n",
        "        self.ffn = PositionwiseFFN(d_model, d_ff)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, memory, self_mask=None):\n",
        "        # 1) masked self-attn in decoder\n",
        "        a, self_attn = self.self_attn(x, x, x, mask=self_mask)\n",
        "        x = self.norm1(x + self.drop(a))\n",
        "\n",
        "        # 2) cross-attn: Q from decoder, K/V from encoder memory\n",
        "        c, cross_attn = self.cross_attn(x, memory, memory, mask=None)\n",
        "        x = self.norm2(x + self.drop(c))\n",
        "\n",
        "        # 3) FFN\n",
        "        f = self.ffn(x)\n",
        "        x = self.norm3(x + self.drop(f))\n",
        "\n",
        "        return x, self_attn, cross_attn\n"
      ],
      "metadata": {
        "id": "6EFPaTt52gdk"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the full encoder-decoder transformer\n",
        "\n",
        "class TinyTransformerSeq2Seq(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.src_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.tgt_emb = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        self.enc_layers = nn.ModuleList([\n",
        "            EncoderBlockPostNorm(d_model, num_heads, d_ff, dropout=dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.dec_layers = nn.ModuleList([\n",
        "            DecoderBlockPostNorm(d_model, num_heads, d_ff, dropout=dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, src_ids, tgt_in_ids):\n",
        "        \"\"\"\n",
        "        src_ids:    (B, S)\n",
        "        tgt_in_ids: (B, T)  (teacher-forced inputs, usually BOS + target[:-1])\n",
        "\n",
        "        returns:\n",
        "          logits: (B, T, vocab_size)\n",
        "          self_attn_last:  (B,h,T,T)\n",
        "          cross_attn_last: (B,h,T,S)\n",
        "        \"\"\"\n",
        "        B, S = src_ids.shape\n",
        "        T = tgt_in_ids.shape[1]\n",
        "        device = src_ids.device\n",
        "\n",
        "        # embeddings + pos enc\n",
        "        src = self.src_emb(src_ids) + sinusoidal_positional_encoding(S, self.d_model, device=device)  # (B,S,d_model)\n",
        "        tgt = self.tgt_emb(tgt_in_ids) + sinusoidal_positional_encoding(T, self.d_model, device=device)  # (B,T,d_model)\n",
        "\n",
        "        # encoder (no mask)\n",
        "        memory = src\n",
        "        for layer in self.enc_layers:\n",
        "            memory = layer(memory)  # (B,S,d_model)\n",
        "\n",
        "        # causal mask for decoder self-attn: (B,1,T,T) broadcast over heads\n",
        "        self_mask = torch.tril(torch.ones(T, T, device=device)).view(1, 1, T, T).expand(B, 1, T, T)\n",
        "\n",
        "        self_attn_last = None\n",
        "        cross_attn_last = None\n",
        "\n",
        "        x = tgt\n",
        "        for layer in self.dec_layers:\n",
        "            x, self_attn_last, cross_attn_last = layer(x, memory, self_mask=self_mask)\n",
        "\n",
        "        logits = self.lm_head(x)  # (B,T,V)\n",
        "        return logits, self_attn_last, cross_attn_last\n",
        "\n"
      ],
      "metadata": {
        "id": "UyV13ESQ2k9p"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a proper seq2seq reverse batch\n",
        "\n",
        "def make_reverse_seq2seq_batch(batch_size, seq_len, vocab_size, device, bos_id=1):\n",
        "    # avoid 0/1 as payload if you want\n",
        "    src = torch.randint(2, vocab_size, (batch_size, seq_len), device=device)\n",
        "    tgt = torch.flip(src, dims=[1])\n",
        "\n",
        "    bos = torch.full((batch_size, 1), bos_id, device=device, dtype=torch.long)\n",
        "    tgt_in = torch.cat([bos, tgt[:, :-1]], dim=1)\n",
        "\n",
        "    return src, tgt_in, tgt\n"
      ],
      "metadata": {
        "id": "IyjWJRB62rmG"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train it (this should climb way above 0.5)\n",
        "torch.manual_seed(0)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)\n",
        "\n",
        "vocab_size = 20\n",
        "seq_len = 32\n",
        "batch_size = 128\n",
        "\n",
        "d_model = 64\n",
        "num_heads = 4\n",
        "d_ff = 128\n",
        "num_layers = 2\n",
        "dropout = 0.0\n",
        "\n",
        "steps = 1200\n",
        "lr = 1e-3\n",
        "\n",
        "model = TinyTransformerSeq2Seq(vocab_size, d_model, num_heads, d_ff, num_layers, dropout=dropout).to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "for step in range(1, steps + 1):\n",
        "    src, tgt_in, tgt_out = make_reverse_seq2seq_batch(batch_size, seq_len, vocab_size, device=device)\n",
        "    logits, self_attn, cross_attn = model(src, tgt_in)\n",
        "\n",
        "    loss = loss_fn(logits.reshape(-1, vocab_size), tgt_out.reshape(-1))\n",
        "\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    if step % 200 == 0:\n",
        "        pred = logits.argmax(dim=-1)\n",
        "        acc = (pred == tgt_out).float().mean().item()\n",
        "        print(f\"step {step:4d} | loss {loss.item():.4f} | acc {acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usY184xM2y9k",
        "outputId": "0034cff7-fd8a-4130-ea49-5d3ac1b5e107"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n",
            "step  200 | loss 0.3264 | acc 0.902\n",
            "step  400 | loss 0.0074 | acc 1.000\n",
            "step  600 | loss 0.0021 | acc 1.000\n",
            "step  800 | loss 0.0013 | acc 1.000\n",
            "step 1000 | loss 0.0009 | acc 1.000\n",
            "step 1200 | loss 0.0005 | acc 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate + inspect cross-attention\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    src, tgt_in, tgt_out = make_reverse_seq2seq_batch(64, seq_len, vocab_size, device=device)\n",
        "    logits, self_attn, cross_attn = model(src, tgt_in)\n",
        "    pred = logits.argmax(dim=-1)\n",
        "    acc = (pred == tgt_out).float().mean().item()\n",
        "\n",
        "print(\"reverse seq2seq token accuracy:\", acc)\n",
        "print(\"self_attn shape:\", self_attn.shape)     # (B,h,T,T)\n",
        "print(\"cross_attn shape:\", cross_attn.shape)   # (B,h,T,S)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkDzPuzw2-Tr",
        "outputId": "e8514a12-7553-4eeb-c782-f4f1653ad90d"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reverse seq2seq token accuracy: 1.0\n",
            "self_attn shape: torch.Size([64, 4, 32, 32])\n",
            "cross_attn shape: torch.Size([64, 4, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results of replication:\n",
        "\n",
        "A working encoderâ€“decoder Transformer that learns reversal perfectly at seq_len=32, and your training curve (0.326 â†’ 0.0005) is exactly what we want in a replication narrative.\n",
        "\n",
        "## What this proves (and how it maps to the paper)\n",
        "\n",
        "Your architecture is correct: encoder self-attn + decoder masked self-attn + cross-attn + FFN + residual + LayerNorm.\n",
        "\n",
        "Your training loop is correct: gradients flow, Wq/Wk/Wv learn.\n",
        "\n",
        "Youâ€™ve empirically shown the â€œshort path lengthâ€ advantage: the decoder can directly attend (via cross-attn) to any encoder position in one hop."
      ],
      "metadata": {
        "id": "IU6u3hfp3ceO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lets show the mechanism (visualize cross-attention)\n",
        "\n",
        "For reversal, the most interesting thing is cross-attention:\n",
        "\n",
        "Decoder position t is trying to output the token that came from encoder position S-1-t.\n",
        "\n",
        "So you expect cross-attention to form a reversal alignment pattern.\n",
        "\n",
        "The following evidence shows: Cross-attention is content-addressed retrieval: the decoder uses queries to retrieve relevant encoder values, and for reversal, it learns a near-deterministic alignment."
      ],
      "metadata": {
        "id": "_9-ah_KF3q_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qomdX7WM42v3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = 0   # pick one example from batch\n",
        "h = 0   # head index\n",
        "\n",
        "print(\"src:\", src[b].tolist())\n",
        "print(\"tgt_out:\", tgt_out[b].tolist())\n",
        "print(\"\\nCross-attn head 0 (T x S):\\n\", cross_attn[b, h].detach().cpu())\n",
        "\n",
        "# For each decoder position t, which encoder position got max attention?\n",
        "align = cross_attn[b, h].argmax(dim=-1).detach().cpu()\n",
        "print(\"\\nArgmax alignment (decoder_pos -> encoder_pos):\", align.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRHgYBcc3ay7",
        "outputId": "911574ff-314c-461b-cfd1-266a8d4503c9"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src: [12, 14, 9, 10, 11, 13, 17, 14, 15, 11, 12, 8, 17, 9, 17, 10, 13, 2, 11, 8, 19, 13, 3, 2, 13, 14, 13, 11, 5, 16, 14, 15]\n",
            "tgt_out: [15, 14, 16, 5, 11, 13, 14, 13, 2, 3, 13, 19, 8, 11, 2, 13, 10, 17, 9, 17, 8, 12, 11, 15, 14, 17, 13, 11, 10, 9, 14, 12]\n",
            "\n",
            "Cross-attn head 0 (T x S):\n",
            " tensor([[6.9417e-09, 1.1615e-09, 8.9411e-12,  ..., 1.6311e-02, 2.2106e-01,\n",
            "         7.6242e-01],\n",
            "        [8.6573e-14, 8.7116e-15, 6.4544e-17,  ..., 9.7794e-02, 4.6557e-01,\n",
            "         4.3030e-01],\n",
            "        [6.1266e-17, 3.8531e-18, 3.4753e-20,  ..., 2.5964e-01, 4.9373e-01,\n",
            "         1.8851e-01],\n",
            "        ...,\n",
            "        [9.9824e-03, 9.1933e-02, 2.8420e-01,  ..., 1.7112e-23, 9.3963e-22,\n",
            "         1.2453e-19],\n",
            "        [1.1765e-01, 4.7845e-01, 2.9743e-01,  ..., 1.3798e-17, 8.1354e-16,\n",
            "         6.5235e-14],\n",
            "        [1.9405e-01, 5.3661e-01, 2.1527e-01,  ..., 1.1635e-16, 7.0131e-15,\n",
            "         6.0266e-13]])\n",
            "\n",
            "Argmax alignment (decoder_pos -> encoder_pos): [31, 30, 30, 28, 27, 27, 25, 24, 23, 23, 21, 21, 20, 18, 17, 16, 15, 15, 13, 13, 11, 11, 9, 8, 7, 7, 5, 4, 3, 3, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6 â€” Small MT (fast, end-to-end pipeline)\n",
        "\n",
        "Goal: Move from toy tasks â†’ a real machine translation task (German â†’ English) using IWSLT14 deâ€“en.\n",
        "Weâ€™re not chasing SOTA yet. Weâ€™re validating the full pipeline end-to-end, then improving it.\n",
        "\n",
        "---\n",
        "\n",
        "## Why this sequence?\n",
        "Machine translation has many moving parts: dataset loading, tokenization, vocabulary, padding/masks, teacher forcing,\n",
        "training loop, decoding, and evaluation metrics. If we try to do everything at once (e.g., SentencePiece + full training),\n",
        "itâ€™s hard to debug. This staged sequence ensures each layer works before adding complexity.\n",
        "\n",
        "---\n",
        "\n",
        "## The fast sequence (recommended)\n",
        "\n",
        "### 1) Load dataset (Hugging Face)\n",
        "**What we do**\n",
        "- Load IWSLT14 deâ€“en from HF.\n",
        "- Inspect one example to confirm the schema:\n",
        "  `{\"translation\": {\"de\": \"...\", \"en\": \"...\"}}`\n",
        "\n",
        "**Why**\n",
        "- HF datasets are reliable and easy to reproduce in Colab.\n",
        "- Verifies we have real parallel data before writing tokenization/model code.\n",
        "\n",
        "**Success criteria**\n",
        "- We can print a sample `de` and `en` sentence.\n",
        "- `train`, `validation`, and `test` splits exist.\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Build a tiny vocab (top N tokens) + `<unk>`\n",
        "**What we do**\n",
        "- Tokenize with a simple baseline tokenizer (whitespace split).\n",
        "- Count tokens on a training slice (e.g., 20k examples).\n",
        "- Build separate vocabularies for source (de) and target (en):\n",
        "  - `<pad>`, `<bos>`, `<eos>`, `<unk>` + top-N tokens\n",
        "\n",
        "**Why**\n",
        "- Fast and transparent: we understand every token â†” id mapping.\n",
        "- Great for debugging padding, masks, and teacher forcing.\n",
        "- This is a **baseline** tokenizer â€” not the final one.\n",
        "\n",
        "**Success criteria**\n",
        "- Vocab sizes match expectations (e.g., ~12k + special tokens).\n",
        "- Unknown words map to `<unk>`.\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Train for 1 epoch on a slice (â‰ˆ 20k) to verify end-to-end\n",
        "**What we do**\n",
        "- Use a small slice of training data to keep iteration fast.\n",
        "- Implement DataLoader + padding.\n",
        "- Train encoderâ€“decoder Transformer with:\n",
        "  - encoder self-attn (full)\n",
        "  - decoder self-attn (causal mask)\n",
        "  - decoder cross-attn (to encoder memory)\n",
        "  - pad masks for encoder keys and decoder keys\n",
        "  - loss ignores `<pad>` via `ignore_index=pad_id`\n",
        "\n",
        "**Why**\n",
        "- Confirms the *entire* forward pass + masking + loss + optimizer are correct.\n",
        "- If loss doesnâ€™t decrease here, we debug before adding tokenizers/long training.\n",
        "\n",
        "**Success criteria**\n",
        "- Training loss decreases over the epoch.\n",
        "- Validation loss is reasonable (not NaN, not exploding).\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Add greedy decoding + BLEU (quick evaluation)\n",
        "**What we do**\n",
        "- Implement greedy decoding:\n",
        "  - start with `<bos>`\n",
        "  - iteratively pick argmax token until `<eos>` or max_len\n",
        "- Compute BLEU using sacreBLEU on a small validation subset.\n",
        "\n",
        "**Why**\n",
        "- Loss alone doesnâ€™t guarantee translations look correct.\n",
        "- BLEU gives a standardized metric for sanity checking and later comparisons.\n",
        "\n",
        "**Success criteria**\n",
        "- Decoding runs end-to-end without crashing.\n",
        "- BLEU is computed successfully (it may be low early â€” thatâ€™s fine).\n",
        "- We can print: source / reference / hypothesis.\n",
        "\n",
        "---\n",
        "\n",
        "### 5) Upgrade tokenizer â†’ SentencePiece and train longer (improvement phase)\n",
        "**What we do**\n",
        "- Replace whitespace tokens with SentencePiece BPE.\n",
        "- Rebuild vocabs and re-train.\n",
        "- Increase training data (full train split) and training time.\n",
        "\n",
        "**Why**\n",
        "- SentencePiece reduces `<unk>` rate and handles morphology/subwords better.\n",
        "- Typically improves BLEU substantially compared to whitespace vocab.\n",
        "\n",
        "**Success criteria**\n",
        "- BLEU improves vs baseline.\n",
        "- Model produces more fluent, less `<unk>`-heavy outputs.\n",
        "\n",
        "---\n",
        "\n",
        "## Notes / Guardrails\n",
        "- This step is about correctness + reproducibility first, then performance.\n",
        "- If something fails, we reduce scope:\n",
        "  - smaller vocab, shorter max_len, fewer layers, smaller data slice.\n",
        "- Always verify:\n",
        "  1) shapes\n",
        "  2) masks (causal + pad)\n",
        "  3) loss decreases on a small slice\n",
        "  4) decoding works before training longer\n"
      ],
      "metadata": {
        "id": "OB_GpIK78SpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install datasets sacrebleu\n"
      ],
      "metadata": {
        "id": "oRnv_BEa8V-s"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"Helsinki-NLP/tatoeba_mt\", \"deu-eng\")  # splits: train/validation/test\n",
        "print(ds)\n",
        "print(ds[\"train\"][0][\"translation\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "ZjWbL0ok8b2f",
        "outputId": "6be76b57-dc4c-4c14-ad61-13aabc65170d"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Dataset scripts are no longer supported, but found tatoeba_mt.py",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-458159289.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Helsinki-NLP/tatoeba_mt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"deu-eng\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# splits: train/validation/test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"translation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1393\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fix_for_backward_compatible_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1133\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m   1029\u001b[0m                         \u001b[0;34mf\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m                     ) from None\n\u001b[0;32m-> 1031\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any data file at {relative_to_absolute_path(path)}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m    987\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 )\n\u001b[0;32m--> 989\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset scripts are no longer supported, but found {filename}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEntryNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m                 \u001b[0;31m# Use the infos from the parquet export except in some cases:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Dataset scripts are no longer supported, but found tatoeba_mt.py"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build tiny vocab (top N) with <unk>\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Special tokens\n",
        "PAD, BOS, EOS, UNK = \"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"\n",
        "\n",
        "pad_id, bos_id, eos_id, unk_id = 0, 1, 2, 3\n",
        "\n",
        "def tokenize_basic(s: str):\n",
        "    return s.strip().split()\n",
        "\n",
        "def build_vocab(examples, lang_key, max_vocab=12000):\n",
        "    counter = Counter()\n",
        "    for ex in examples:\n",
        "        text = ex[\"translation\"][lang_key]\n",
        "        counter.update(tokenize_basic(text))\n",
        "    most_common = counter.most_common(max_vocab - 4)\n",
        "\n",
        "    itos = [PAD, BOS, EOS, UNK] + [w for w, _ in most_common]\n",
        "    stoi = {w:i for i,w in enumerate(itos)}\n",
        "    return stoi, itos\n",
        "\n",
        "# Take a slice of training for speed (20k)\n",
        "train_slice = ds[\"train\"].select(range(20000))\n",
        "\n",
        "src_stoi, src_itos = build_vocab(train_slice, \"de\", max_vocab=12000)\n",
        "tgt_stoi, tgt_itos = build_vocab(train_slice, \"en\", max_vocab=12000)\n",
        "\n",
        "print(\"src vocab size:\", len(src_itos))\n",
        "print(\"tgt vocab size:\", len(tgt_itos))\n"
      ],
      "metadata": {
        "id": "Vlhzj_Iy833o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numericalize + DataLoader (padding)\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)\n",
        "\n",
        "def encode(text, stoi, max_len):\n",
        "    toks = tokenize_basic(text)[:max_len-2]\n",
        "    ids = [bos_id] + [stoi.get(t, unk_id) for t in toks] + [eos_id]\n",
        "    return ids\n",
        "\n",
        "def collate_fn(batch, max_src_len=64, max_tgt_len=64):\n",
        "    # batch: list of dicts with \"translation\"\n",
        "    src_ids_list = []\n",
        "    tgt_in_list = []\n",
        "    tgt_out_list = []\n",
        "\n",
        "    for ex in batch:\n",
        "        src_text = ex[\"translation\"][\"de\"]\n",
        "        tgt_text = ex[\"translation\"][\"en\"]\n",
        "\n",
        "        src_ids = encode(src_text, src_stoi, max_src_len)              # [BOS ... EOS]\n",
        "        tgt_ids = encode(tgt_text, tgt_stoi, max_tgt_len)              # [BOS ... EOS]\n",
        "\n",
        "        # Teacher forcing:\n",
        "        # tgt_in  = [BOS, w1, w2, ...]\n",
        "        # tgt_out = [w1,  w2, ... , EOS]\n",
        "        tgt_in  = tgt_ids[:-1]\n",
        "        tgt_out = tgt_ids[1:]\n",
        "\n",
        "        src_ids_list.append(src_ids)\n",
        "        tgt_in_list.append(tgt_in)\n",
        "        tgt_out_list.append(tgt_out)\n",
        "\n",
        "    def pad_to_max(seqs, pad_value):\n",
        "        max_len = max(len(s) for s in seqs)\n",
        "        out = torch.full((len(seqs), max_len), pad_value, dtype=torch.long)\n",
        "        for i,s in enumerate(seqs):\n",
        "            out[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n",
        "        return out\n",
        "\n",
        "    src = pad_to_max(src_ids_list, pad_id)       # (B, S)\n",
        "    tgt_in = pad_to_max(tgt_in_list, pad_id)     # (B, T)\n",
        "    tgt_out = pad_to_max(tgt_out_list, pad_id)   # (B, T)\n",
        "\n",
        "    return src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
        "\n",
        "train_loader = DataLoader(train_slice, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(ds[\"validation\"], batch_size=64, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "EepI3g3i9JUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update your model to support padding masks (important for MT)\n",
        "\n",
        "# Your seq2seq toy model worked because there was no padding. MT batches have padding, so we must block attention to PAD tokens.\n",
        "# Drop-in patch: make your decoder block accept cross_mask + self_mask\n",
        "#If you already have MultiHeadAttentionGeneral, PositionwiseFFN, and your scaled_dot_product_attention,\n",
        "# paste this MT-ready model (itâ€™s basically your Option B, but with pad-masks):\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "def sinusoidal_positional_encoding(T: int, d_model: int, device):\n",
        "    pe = torch.zeros(T, d_model, device=device)\n",
        "    position = torch.arange(0, T, device=device).unsqueeze(1).float()\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2, device=device).float() * (-math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    return pe.unsqueeze(0)  # (1,T,d_model)\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    d_k = Q.size(-1)\n",
        "    scores = Q @ K.transpose(-2, -1) / (d_k ** 0.5)  # (B,h,Tq,Tk)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    attn = F.softmax(scores, dim=-1)\n",
        "    out = attn @ V\n",
        "    return out, attn\n",
        "\n",
        "class PositionwiseFFN(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.relu(self.fc1(x)))\n",
        "\n",
        "class MultiHeadAttentionGeneral(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.h = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.Wq = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.Wk = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.Wv = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.Wo = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, Q_in, K_in, V_in, mask=None):\n",
        "        B, Tq, _ = Q_in.shape\n",
        "        Tk = K_in.shape[1]\n",
        "\n",
        "        Q = self.Wq(Q_in)\n",
        "        K = self.Wk(K_in)\n",
        "        V = self.Wv(V_in)\n",
        "\n",
        "        Q = Q.view(B, Tq, self.h, self.d_k).transpose(1, 2)  # (B,h,Tq,d_k)\n",
        "        K = K.view(B, Tk, self.h, self.d_k).transpose(1, 2)  # (B,h,Tk,d_k)\n",
        "        V = V.view(B, Tk, self.h, self.d_k).transpose(1, 2)  # (B,h,Tk,d_k)\n",
        "\n",
        "        out, attn = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
        "        out = self.drop(out)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, Tq, self.d_model)\n",
        "        out = self.Wo(out)\n",
        "        return out, attn\n",
        "\n",
        "class EncoderBlockPostNorm(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttentionGeneral(d_model, num_heads, dropout=dropout)\n",
        "        self.ffn = PositionwiseFFN(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_mask=None):\n",
        "        a, _ = self.self_attn(x, x, x, mask=enc_mask)\n",
        "        x = self.norm1(x + self.drop(a))\n",
        "        f = self.ffn(x)\n",
        "        x = self.norm2(x + self.drop(f))\n",
        "        return x\n",
        "\n",
        "class DecoderBlockPostNorm(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttentionGeneral(d_model, num_heads, dropout=dropout)\n",
        "        self.cross_attn = MultiHeadAttentionGeneral(d_model, num_heads, dropout=dropout)\n",
        "        self.ffn = PositionwiseFFN(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, memory, self_mask=None, cross_mask=None):\n",
        "        a, self_attn = self.self_attn(x, x, x, mask=self_mask)\n",
        "        x = self.norm1(x + self.drop(a))\n",
        "\n",
        "        c, cross_attn = self.cross_attn(x, memory, memory, mask=cross_mask)\n",
        "        x = self.norm2(x + self.drop(c))\n",
        "\n",
        "        f = self.ffn(x)\n",
        "        x = self.norm3(x + self.drop(f))\n",
        "        return x, self_attn, cross_attn\n",
        "\n",
        "class TinyTransformerSeq2SeqMT(nn.Module):\n",
        "    def __init__(self, src_vocab, tgt_vocab, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.src_emb = nn.Embedding(src_vocab, d_model)\n",
        "        self.tgt_emb = nn.Embedding(tgt_vocab, d_model)\n",
        "        self.enc_layers = nn.ModuleList([EncoderBlockPostNorm(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dec_layers = nn.ModuleList([DecoderBlockPostNorm(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.lm_head = nn.Linear(d_model, tgt_vocab, bias=False)\n",
        "\n",
        "    def forward(self, src_ids, tgt_in_ids, src_key_mask, tgt_key_mask):\n",
        "        # src_ids: (B,S), tgt_in_ids: (B,T)\n",
        "        B, S = src_ids.shape\n",
        "        T = tgt_in_ids.shape[1]\n",
        "        device = src_ids.device\n",
        "\n",
        "        src = self.src_emb(src_ids) + sinusoidal_positional_encoding(S, self.d_model, device=device)\n",
        "        tgt = self.tgt_emb(tgt_in_ids) + sinusoidal_positional_encoding(T, self.d_model, device=device)\n",
        "\n",
        "        # Encoder mask: block attending TO padded source positions\n",
        "        # src_key_mask: (B,S) True for real tokens\n",
        "        enc_mask = src_key_mask.view(B, 1, 1, S).expand(B, 1, S, S)\n",
        "\n",
        "        memory = src\n",
        "        for layer in self.enc_layers:\n",
        "            memory = layer(memory, enc_mask=enc_mask)\n",
        "\n",
        "        # Decoder self-attn mask = causal AND \"donâ€™t attend to PAD keys\"\n",
        "        causal = torch.tril(torch.ones(T, T, device=device)).view(1, 1, T, T)\n",
        "        dec_key = tgt_key_mask.view(B, 1, 1, T).expand(B, 1, T, T)\n",
        "        self_mask = causal.expand(B, 1, T, T) * dec_key\n",
        "\n",
        "        # Cross mask: decoder queries can only attend to non-pad encoder keys\n",
        "        cross_mask = src_key_mask.view(B, 1, 1, S).expand(B, 1, T, S)\n",
        "\n",
        "        self_attn_last = None\n",
        "        cross_attn_last = None\n",
        "        x = tgt\n",
        "        for layer in self.dec_layers:\n",
        "            x, self_attn_last, cross_attn_last = layer(x, memory, self_mask=self_mask, cross_mask=cross_mask)\n",
        "\n",
        "        logits = self.lm_head(x)\n",
        "        return logits, self_attn_last, cross_attn_last\n",
        "\n"
      ],
      "metadata": {
        "id": "GTIVkw7l9QSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for 1 epoch on a 20k slice (verify end-to-end)\n",
        "\n",
        "import torch\n",
        "\n",
        "# Hyperparams (small but real)\n",
        "d_model = 128\n",
        "num_heads = 4\n",
        "d_ff = 256\n",
        "num_layers = 2\n",
        "dropout = 0.1\n",
        "\n",
        "model = TinyTransformerSeq2SeqMT(\n",
        "    src_vocab=len(src_itos),\n",
        "    tgt_vocab=len(tgt_itos),\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    d_ff=d_ff,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout\n",
        ").to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    model.train() if train else model.eval()\n",
        "    total_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for src, tgt_in, tgt_out in loader:\n",
        "        src_key_mask = (src != pad_id)      # (B,S)\n",
        "        tgt_key_mask = (tgt_in != pad_id)   # (B,T)\n",
        "\n",
        "        logits, _, _ = model(src, tgt_in, src_key_mask, tgt_key_mask)\n",
        "        loss = loss_fn(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    return total_loss / max(1, n_batches)\n",
        "\n",
        "# 1 epoch on train slice\n",
        "train_loss = run_epoch(train_loader, train=True)\n",
        "val_loss = run_epoch(val_loader, train=False)\n",
        "print(\"train_loss:\", train_loss)\n",
        "print(\"val_loss:\", val_loss)\n",
        "\n"
      ],
      "metadata": {
        "id": "6tgYpjVo9hav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedy decoding and sacreBLEU\n",
        "\n",
        "def decode_ids(ids, itos):\n",
        "    # stop at EOS, skip special tokens\n",
        "    out = []\n",
        "    for i in ids:\n",
        "        if i == eos_id:\n",
        "            break\n",
        "        if i in (pad_id, bos_id):\n",
        "            continue\n",
        "        out.append(itos[i] if i < len(itos) else UNK)\n",
        "    return \" \".join(out)\n",
        "\n",
        "@torch.no_grad()\n",
        "def greedy_translate(src_ids, max_len=64):\n",
        "    model.eval()\n",
        "    B, S = src_ids.shape\n",
        "    src_key_mask = (src_ids != pad_id)\n",
        "\n",
        "    # start with BOS\n",
        "    tgt_in = torch.full((B, 1), bos_id, dtype=torch.long, device=device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        tgt_key_mask = (tgt_in != pad_id)\n",
        "        logits, _, _ = model(src_ids, tgt_in, src_key_mask, tgt_key_mask)\n",
        "        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)  # (B,1)\n",
        "        tgt_in = torch.cat([tgt_in, next_token], dim=1)\n",
        "        if (next_token == eos_id).all():\n",
        "            break\n",
        "\n",
        "    return tgt_in  # includes BOS + generated tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "IR95MkJ89s0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute corpus BLUE with SacreBLEU\n",
        "\n",
        "from sacrebleu.metrics import BLEU\n",
        "\n",
        "bleu = BLEU()  # standardized BLEU scorer :contentReference[oaicite:3]{index=3}\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_bleu(loader, num_batches=20):\n",
        "    model.eval()\n",
        "    hyps = []\n",
        "    refs = []\n",
        "\n",
        "    for i, (src, tgt_in, tgt_out) in enumerate(loader):\n",
        "        if i >= num_batches:\n",
        "            break\n",
        "\n",
        "        pred_ids = greedy_translate(src, max_len=64)  # (B, <=max_len)\n",
        "\n",
        "        for b in range(src.size(0)):\n",
        "            hyp = decode_ids(pred_ids[b].tolist(), tgt_itos)\n",
        "\n",
        "            # rebuild reference string from tgt_out (shifted)\n",
        "            ref = decode_ids(tgt_out[b].tolist(), tgt_itos)\n",
        "\n",
        "            hyps.append(hyp)\n",
        "            refs.append(ref)\n",
        "\n",
        "    score = bleu.corpus_score(hyps, [refs])\n",
        "    return score\n",
        "\n",
        "bleu_score = eval_bleu(val_loader, num_batches=20)\n",
        "print(\"BLEU on validation (subset):\", bleu_score)\n"
      ],
      "metadata": {
        "id": "a_mPmSSE9z7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a few qualitative examples from validation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    batch = next(iter(val_loader))\n",
        "    src, tgt_in, tgt_out = batch\n",
        "\n",
        "    pred_ids = greedy_translate(src[:3], max_len=64)\n",
        "\n",
        "    for i in range(3):\n",
        "        # Source (German)\n",
        "        src_text = decode_ids(src[i].tolist(), src_itos)\n",
        "        # Reference (English)\n",
        "        ref_text = decode_ids(tgt_out[i].tolist(), tgt_itos)\n",
        "        # Hypothesis (English)\n",
        "        hyp_text = decode_ids(pred_ids[i].tolist(), tgt_itos)\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        print(\"SRC (de):\", src_text)\n",
        "        print(\"REF (en):\", ref_text)\n",
        "        print(\"HYP (en):\", hyp_text)\n"
      ],
      "metadata": {
        "id": "TYEQU1aa-Wkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b53bdebf"
      },
      "source": [
        "\n",
        "\n",
        "## âš ï¸ Diagnosis: Need for SentencePiece Tokenization\n",
        "\n",
        "The current baseline tokenizer (simple whitespace splitting) has allowed us to verify the Transformer architecture and training pipeline. However, it exhibits significant limitations that hinder translation quality, particularly evident in the qualitative examples:\n",
        "\n",
        "1.  **High `<unk>` Rate**:\n",
        "    *   Many words, especially less frequent ones or inflected forms, are not present in our limited 12,000-token vocabulary derived from a small training slice. These words are replaced by the `<unk>` token.\n",
        "    *   **Problem**: When a model sees `<unk>` tokens in the input or is forced to generate them in the output, it loses semantic information. A translation like \"A `<unk>` strategy to counter the `<unk>` of `<unk>`\" is practically meaningless because key nouns and adjectives are unknown. This severely limits the model's ability to produce accurate or fluent translations.\n",
        "\n",
        "2.  **Inflectional and Compound Word Challenges (German Specific)**:\n",
        "    *   German is a highly inflected language (e.g., different endings for cases, genders) and frequently forms long compound words (e.g., \"Krankenhaus\", \"DonaudampfschifffahrtsgesellschaftskapitÃ¤n\").\n",
        "    *   **Problem**: A whitespace tokenizer treats each unique inflected form or compound as a completely new word. This drastically increases vocabulary size (even with frequency-based truncation, many are still `<unk>`), and the model cannot easily learn relationships between \"gehen\" (to go), \"geht\" (goes), \"gegangen\" (gone). Similarly, common sub-word units within compounds (like \"kranken\", \"haus\") are lost, forcing the model to memorize entire long words.\n",
        "\n",
        "3.  **Limited Vocabulary Size & Coverage**:\n",
        "    *   Even with 12,000 tokens, a vocabulary built on raw words often struggles to cover the diversity of natural language, especially for morphologically rich languages. Many infrequent but important words will always fall outside the vocabulary.\n",
        "    *   **Problem**: This directly leads to the high `<unk>` rate observed, making it impossible for the model to translate sentences containing out-of-vocabulary words accurately.\n",
        "\n",
        "---\n",
        "\n",
        "## âœ¨ Solution: SentencePiece with Byte Pair Encoding (BPE)\n",
        "\n",
        "SentencePiece, specifically using the Byte Pair Encoding (BPE) algorithm, offers a robust solution to these problems:\n",
        "\n",
        "1.  **Subword Tokenization**:\n",
        "    *   Instead of splitting into full words, BPE breaks down rare or unknown words into smaller, frequently occurring subword units. For example, \"unbelievable\" might become \"un\", \"believe\", \"able\". In German, \"Krankenhaus\" could become \"Kranken\" and \"haus\".\n",
        "    *   **Benefit**: This allows the model to handle unseen words by composing them from known subword units. The number of `<unk>` tokens is drastically reduced, as almost any word can be represented.\n",
        "\n",
        "2.  **Manages Inflection and Compounding**:\n",
        "    *   By breaking down words into subwords, BPE naturally handles different inflections and compound words more effectively. The model can learn the meaning of common prefixes, suffixes, and word stems.\n",
        "    *   **Benefit**: This significantly improves the model's ability to generalize to new or complex word forms, especially in languages like German.\n",
        "\n",
        "3.  **Smaller, More Representative Vocabulary**:\n",
        "    *   A SentencePiece vocabulary (typically 32k or 64k tokens) consists of common words and frequently occurring subword units. This vocabulary is much more efficient than a word-level vocabulary of the same size.\n",
        "    *   **Benefit**: It provides better coverage of the language with a manageable vocabulary size, allowing the model to learn more robust representations.\n",
        "\n",
        "4.  **Language-Agnostic and Unified Processing**:\n",
        "    *   SentencePiece trains directly on raw text without requiring pre-tokenized input or language-specific rules. It handles whitespace consistently, often treating it as a special character.\n",
        "    *   **Benefit**: This simplifies the preprocessing pipeline and ensures consistent tokenization across different languages.\n",
        "\n",
        "**In summary, upgrading to SentencePiece BPE is a crucial step to move beyond the limitations of simple whitespace tokenization, enabling the Transformer model to learn more effective and generalizable representations for machine translation, thereby improving overall translation quality and significantly reducing the \"noise\" introduced by `<unk>` tokens.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39c9fae0"
      },
      "source": [
        "pip install -q sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a9db3fd",
        "outputId": "a14caca0-1821-4be3-a3b0-73f6bda315c8"
      },
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Load the trained SentencePiece models\n",
        "sp_de = spm.SentencePieceProcessor(model_file='de_sp.model')\n",
        "sp_en = spm.SentencePieceProcessor(model_file='en_sp.model')\n",
        "\n",
        "# Define special token IDs based on SentencePiece defaults\n",
        "# SentencePiece typically uses 0 for <unk>, 1 for <s>, 2 for </s>\n",
        "# We will use 0 for padding as well, a common practice where padding takes the <unk> ID\n",
        "pad_id = sp_en.unk_id() # Use <unk> id for padding\n",
        "bos_id = sp_en.bos_id()\n",
        "eos_id = sp_en.eos_id()\n",
        "unk_id = sp_en.unk_id()\n",
        "\n",
        "print(f\"SentencePiece: pad_id={pad_id}, bos_id={bos_id}, eos_id={eos_id}, unk_id={unk_id}\")"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentencePiece: pad_id=0, bos_id=1, eos_id=2, unk_id=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "809bee90",
        "outputId": "99bfae88-b712-46e8-c769-296a2ecd4c32"
      },
      "source": [
        "# New encode function using SentencePiece\n",
        "def encode_spm(text, sp_model, max_len):\n",
        "    # sp_model.encode_as_ids adds <s> and </s> by default if not suppressed\n",
        "    ids = sp_model.encode_as_ids(text)\n",
        "\n",
        "    # Ensure BOS and EOS are correctly handled.\n",
        "    # SentencePiece encode_as_ids usually includes <s> and </s> by default.\n",
        "    # We need to make sure they match our expected bos_id and eos_id consistently.\n",
        "    # sp_model.encode_as_ids(text, add_bos=True, add_eos=True) is the default behavior\n",
        "\n",
        "    # Clip sequence to max_len (including BOS/EOS if present)\n",
        "    if len(ids) > max_len:\n",
        "        # Preserve BOS and EOS if they are automatically added, or handle manually\n",
        "        # Assuming sp_model.encode_as_ids already added them, we clip from the middle if necessary\n",
        "        # For simplicity, if length exceeds max_len, we just truncate, ensuring BOS/EOS are at ends\n",
        "        # A more robust solution might ensure BOS/EOS are always present after clipping\n",
        "        ids = ids[:max_len-1] + [sp_model.eos_id()] # Truncate and ensure EOS\n",
        "    elif len(ids) < max_len and ids[-1] != sp_model.eos_id():\n",
        "        # Add EOS if not present (should be by default, but double check)\n",
        "        ids.append(sp_model.eos_id())\n",
        "\n",
        "    return ids\n",
        "\n",
        "# Updated collate_fn to use SentencePiece\n",
        "def collate_fn_spm(batch, max_src_len=64, max_tgt_len=64):\n",
        "    src_ids_list = []\n",
        "    tgt_in_list = []\n",
        "    tgt_out_list = []\n",
        "\n",
        "    for ex in batch:\n",
        "        src_text = ex[\"translation\"][\"de\"]\n",
        "        tgt_text = ex[\"translation\"][\"en\"]\n",
        "\n",
        "        # Encode using SentencePiece\n",
        "        src_ids = encode_spm(src_text, sp_de, max_src_len) # Contains <s>...</s>\n",
        "        tgt_ids = encode_spm(tgt_text, sp_en, max_tgt_len) # Contains <s>...</s>\n",
        "\n",
        "        # Teacher forcing:\n",
        "        # tgt_in  = [BOS, w1, w2, ...]\n",
        "        # tgt_out = [w1,  w2, ... , EOS]\n",
        "        # SentencePiece encode_as_ids includes <s> and </s>.\n",
        "        # We need to adjust for tgt_in/tgt_out if sp_model.encode_as_ids adds them.\n",
        "        # Default behavior: sp_model.encode_as_ids adds <s> and </s>\n",
        "        # So, src_ids will be [<s>, token_ids..., </s>]\n",
        "        # tgt_ids will be [<s>, token_ids..., </s>]\n",
        "\n",
        "        # If sp_model.encode_as_ids(text) results in [<s>, tok1, tok2, ..., tokN, </s>]\n",
        "        # then for training, tgt_in should be [<s>, tok1, ..., tokN]\n",
        "        # and tgt_out should be [tok1, ..., tokN, </s>]\n",
        "        tgt_in  = tgt_ids[:-1] # Remove the final </s> from tgt_ids for input\n",
        "        tgt_out = tgt_ids[1:]  # Remove the initial <s> from tgt_ids for target\n",
        "\n",
        "        src_ids_list.append(src_ids)\n",
        "        tgt_in_list.append(tgt_in)\n",
        "        tgt_out_list.append(tgt_out)\n",
        "\n",
        "    def pad_to_max(seqs, pad_value):\n",
        "        max_len = max(len(s) for s in seqs)\n",
        "        out = torch.full((len(seqs), max_len), pad_value, dtype=torch.long)\n",
        "        for i,s in enumerate(seqs):\n",
        "            out[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n",
        "        return out\n",
        "\n",
        "    src = pad_to_max(src_ids_list, pad_id)       # (B, S)\n",
        "    tgt_in = pad_to_max(tgt_in_list, pad_id)     # (B, T)\n",
        "    tgt_out = pad_to_max(tgt_out_list, pad_id)   # (B, T)\n",
        "\n",
        "    return src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
        "\n",
        "# Re-create DataLoaders with the new collate_fn_spm\n",
        "train_loader_spm = DataLoader(train_slice, batch_size=64, shuffle=True, collate_fn=lambda b: collate_fn_spm(b, max_src_len=64, max_tgt_len=64))\n",
        "val_loader_spm = DataLoader(ds[\"validation\"], batch_size=64, shuffle=False, collate_fn=lambda b: collate_fn_spm(b, max_src_len=64, max_tgt_len=64))\n",
        "\n",
        "# Get updated vocabulary sizes\n",
        "src_vocab_size_spm = sp_de.get_piece_size()\n",
        "tgt_vocab_size_spm = sp_en.get_piece_size()\n",
        "\n",
        "print(f\"Updated Source Vocab Size (SentencePiece): {src_vocab_size_spm}\")\n",
        "print(f\"Updated Target Vocab Size (SentencePiece): {tgt_vocab_size_spm}\")"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Source Vocab Size (SentencePiece): 16000\n",
            "Updated Target Vocab Size (SentencePiece): 16000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "4c3ba50d",
        "outputId": "909c2ba5-e5c8-44d3-e9f1-ace7dc5c2d50"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Hyperparams (can be adjusted for better performance, but keeping similar for now)\n",
        "d_model = 128\n",
        "num_heads = 4\n",
        "d_ff = 256\n",
        "num_layers = 2\n",
        "dropout = 0.1 # Re-introduce dropout\n",
        "\n",
        "model_spm = TinyTransformerSeq2SeqMT(\n",
        "    src_vocab=src_vocab_size_spm,\n",
        "    tgt_vocab=tgt_vocab_size_spm,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    d_ff=d_ff,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout\n",
        ").to(device)\n",
        "\n",
        "opt_spm = torch.optim.AdamW(model_spm.parameters(), lr=3e-4)\n",
        "loss_fn_spm = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
        "\n",
        "# Training loop for the SentencePiece-enabled model\n",
        "def run_epoch_spm(loader, train=True):\n",
        "    model_spm.train() if train else model_spm.eval()\n",
        "    total_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for src, tgt_in, tgt_out in loader:\n",
        "        src_key_mask = (src != pad_id)      # (B,S)\n",
        "        tgt_key_mask = (tgt_in != pad_id)   # (B,T)\n",
        "\n",
        "        logits, _, _ = model_spm(src, tgt_in, src_key_mask, tgt_key_mask)\n",
        "        loss = loss_fn_spm(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "\n",
        "        if train:\n",
        "            opt_spm.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            opt_spm.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    return total_loss / max(1, n_batches)\n",
        "\n",
        "print(\"Starting training with SentencePiece tokenization...\")\n",
        "# Train for a few epochs\n",
        "num_epochs = 3\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss_spm = run_epoch_spm(train_loader_spm, train=True)\n",
        "    val_loss_spm = run_epoch_spm(val_loader_spm, train=False)\n",
        "    print(f\"Epoch {epoch:2d} | Train Loss: {train_loss_spm:.4f} | Val Loss: {val_loss_spm:.4f}\")"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training with SentencePiece tokenization...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "EncoderBlockPostNorm.forward() got an unexpected keyword argument 'enc_mask'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-575735074.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mtrain_loss_spm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch_spm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader_spm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mval_loss_spm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch_spm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader_spm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch:2d} | Train Loss: {train_loss_spm:.4f} | Val Loss: {val_loss_spm:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-575735074.py\u001b[0m in \u001b[0;36mrun_epoch_spm\u001b[0;34m(loader, train)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtgt_key_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtgt_in\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpad_id\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (B,T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_spm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn_spm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2502669828.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src_ids, tgt_in_ids, src_key_mask, tgt_key_mask)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Decoder self-attn mask = causal AND \"donâ€™t attend to PAD keys\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: EncoderBlockPostNorm.forward() got an unexpected keyword argument 'enc_mask'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc27a8f2"
      },
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Load the trained SentencePiece models\n",
        "sp_de = spm.SentencePieceProcessor(model_file='de_sp.model')\n",
        "sp_en = spm.SentencePieceProcessor(model_file='en_sp.model')\n",
        "\n",
        "# Define special token IDs based on SentencePiece defaults\n",
        "# SentencePiece typically uses 0 for <unk>, 1 for <s>, 2 for </s>\n",
        "# We will use 0 for padding as well, a common practice where padding takes the <unk> ID\n",
        "pad_id = sp_en.unk_id() # Use <unk> id for padding\n",
        "bos_id = sp_en.bos_id()\n",
        "eos_id = sp_en.eos_id()\n",
        "unk_id = sp_en.unk_id()\n",
        "\n",
        "print(f\"SentencePiece: pad_id={pad_id}, bos_id={bos_id}, eos_id={eos_id}, unk_id={unk_id}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72aab66c"
      },
      "source": [
        "# New encode function using SentencePiece\n",
        "def encode_spm(text, sp_model, max_len):\n",
        "    # sp_model.encode_as_ids adds <s> and </s> by default if not suppressed\n",
        "    ids = sp_model.encode_as_ids(text)\n",
        "\n",
        "    # Ensure BOS and EOS are correctly handled.\n",
        "    # SentencePiece encode_as_ids usually includes <s> and </s> by default.\n",
        "    # We need to make sure they match our expected bos_id and eos_id consistently.\n",
        "    # sp_model.encode_as_ids(text, add_bos=True, add_eos=True) is the default behavior\n",
        "\n",
        "    # Clip sequence to max_len (including BOS/EOS if present)\n",
        "    if len(ids) > max_len:\n",
        "        # Preserve BOS and EOS if they are automatically added, or handle manually\n",
        "        # Assuming sp_model.encode_as_ids already added them, we clip from the middle if necessary\n",
        "        # For simplicity, if length exceeds max_len, we just truncate, ensuring BOS/EOS are at ends\n",
        "        # A more robust solution might ensure BOS/EOS are always present after clipping\n",
        "        ids = ids[:max_len-1] + [sp_model.eos_id()] # Truncate and ensure EOS\n",
        "    elif len(ids) < max_len and ids[-1] != sp_model.eos_id():\n",
        "        # Add EOS if not present (should be by default, but double check)\n",
        "        ids.append(sp_model.eos_id())\n",
        "\n",
        "    return ids\n",
        "\n",
        "# Updated collate_fn to use SentencePiece\n",
        "def collate_fn_spm(batch, max_src_len=64, max_tgt_len=64):\n",
        "    src_ids_list = []\n",
        "    tgt_in_list = []\n",
        "    tgt_out_list = []\n",
        "\n",
        "    for ex in batch:\n",
        "        src_text = ex[\"translation\"][\"de\"]\n",
        "        tgt_text = ex[\"translation\"][\"en\"]\n",
        "\n",
        "        # Encode using SentencePiece\n",
        "        src_ids = encode_spm(src_text, sp_de, max_src_len) # Contains <s>...</s>\n",
        "        tgt_ids = encode_spm(tgt_text, sp_en, max_tgt_len) # Contains <s>...</s>\n",
        "\n",
        "        # Teacher forcing:\n",
        "        # tgt_in  = [BOS, w1, w2, ...]\n",
        "        # tgt_out = [w1,  w2, ... , EOS]\n",
        "        # SentencePiece encode_as_ids includes <s> and </s>.\n",
        "        # We need to adjust for tgt_in/tgt_out if sp_model.encode_as_ids adds them.\n",
        "        # Default behavior: sp_model.encode_as_ids(text) results in [<s>, tok1, tok2, ..., tokN, </s>]\n",
        "        # then for training, tgt_in should be [<s>, tok1, ..., tokN]\n",
        "        # and tgt_out should be [tok1, ..., tokN, </s>]\n",
        "        tgt_in  = tgt_ids[:-1] # Remove the final </s> from tgt_ids for input\n",
        "        tgt_out = tgt_ids[1:]  # Remove the initial <s> from tgt_ids for target\n",
        "\n",
        "        src_ids_list.append(src_ids)\n",
        "        tgt_in_list.append(tgt_in)\n",
        "        tgt_out_list.append(tgt_out)\n",
        "\n",
        "    def pad_to_max(seqs, pad_value):\n",
        "        max_len = max(len(s) for s in seqs)\n",
        "        out = torch.full((len(seqs), max_len), pad_value, dtype=torch.long)\n",
        "        for i,s in enumerate(seqs):\n",
        "            out[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n",
        "        return out\n",
        "\n",
        "    src = pad_to_max(src_ids_list, pad_id)       # (B, S)\n",
        "    tgt_in = pad_to_max(tgt_in_list, pad_id)     # (B, T)\n",
        "    tgt_out = pad_to_max(tgt_out_list, pad_id)   # (B, T)\n",
        "\n",
        "    return src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
        "\n",
        "# Re-create DataLoaders with the new collate_fn_spm\n",
        "train_loader_spm = DataLoader(train_slice, batch_size=64, shuffle=True, collate_fn=lambda b: collate_fn_spm(b, max_src_len=64, max_tgt_len=64))\n",
        "val_loader_spm = DataLoader(ds[\"validation\"], batch_size=64, shuffle=False, collate_fn=lambda b: collate_fn_spm(b, max_src_len=64, max_tgt_len=64))\n",
        "\n",
        "# Get updated vocabulary sizes\n",
        "src_vocab_size_spm = sp_de.get_piece_size()\n",
        "tgt_vocab_size_spm = sp_en.get_piece_size()\n",
        "\n",
        "print(f\"Updated Source Vocab Size (SentencePiece): {src_vocab_size_spm}\")\n",
        "print(f\"Updated Target Vocab Size (SentencePiece): {tgt_vocab_size_spm}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "564d1fcf"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Hyperparams (can be adjusted for better performance, but keeping similar for now)\n",
        "d_model = 128\n",
        "num_heads = 4\n",
        "d_ff = 256\n",
        "num_layers = 2\n",
        "dropout = 0.1 # Re-introduce dropout\n",
        "\n",
        "model_spm = TinyTransformerSeq2SeqMT(\n",
        "    src_vocab=src_vocab_size_spm,\n",
        "    tgt_vocab=tgt_vocab_size_spm,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    d_ff=d_ff,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout\n",
        ").to(device)\n",
        "\n",
        "opt_spm = torch.optim.AdamW(model_spm.parameters(), lr=3e-4)\n",
        "loss_fn_spm = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
        "\n",
        "# Training loop for the SentencePiece-enabled model\n",
        "def run_epoch_spm(loader, train=True):\n",
        "    model_spm.train() if train else model_spm.eval()\n",
        "    total_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for src, tgt_in, tgt_out in loader:\n",
        "        src_key_mask = (src != pad_id)      # (B,S)\n",
        "        tgt_key_mask = (tgt_in != pad_id)   # (B,T)\n",
        "\n",
        "        logits, _, _ = model_spm(src, tgt_in, src_key_mask, tgt_key_mask)\n",
        "        loss = loss_fn_spm(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "\n",
        "        if train:\n",
        "            opt_spm.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            opt_spm.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    return total_loss / max(1, n_batches)\n",
        "\n",
        "print(\"Starting training with SentencePiece tokenization...\")\n",
        "# Train for a few epochs\n",
        "num_epochs = 3\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss_spm = run_epoch_spm(train_loader_spm, train=True)\n",
        "    val_loss_spm = run_epoch_spm(val_loader_spm, train=False)\n",
        "    print(f\"Epoch {epoch:2d} | Train Loss: {train_loss_spm:.4f} | Val Loss: {val_loss_spm:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "273e9aed"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to extract German sentences from the `train` split of the dataset and save them to a temporary file, with each sentence on a new line, as per the second instruction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0cea0c7"
      },
      "source": [
        "import os\n",
        "\n",
        "de_sentences = [example[\"translation\"][\"de\"] for example in ds[\"train\"]]\n",
        "with open(\"de_sentences.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for sentence in de_sentences:\n",
        "        f.write(sentence + \"\\n\")\n",
        "\n",
        "print(\"German sentences extracted and saved to 'de_sentences.txt'.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0097450a"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the German sentences are saved, I will train the SentencePiece model for German using the specified parameters, as per the third instruction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d63cb16e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c87c3a5-52c0-4523-ad33-e0ac45a3c0ea"
      },
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    '--input=de_sentences.txt --model_prefix=de_sp --vocab_size=16000 --model_type=bpe'\n",
        ")\n",
        "\n",
        "print(\"SentencePiece model for German (de_sp.model, de_sp.vocab) trained.\")"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentencePiece model for German (de_sp.model, de_sp.vocab) trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bbde98d",
        "outputId": "a71c5c67-634a-4387-f210-bad741be9d81"
      },
      "source": [
        "en_sentences = [example[\"translation\"][\"en\"] for example in ds[\"train\"]]\n",
        "with open(\"en_sentences.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for sentence in en_sentences:\n",
        "        f.write(sentence + \"\\n\")\n",
        "\n",
        "print(\"English sentences extracted and saved to 'en_sentences.txt'.\")"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English sentences extracted and saved to 'en_sentences.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2474def"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the English sentences are saved, I will train the SentencePiece model for English using the specified parameters, as per the third instruction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3546a803",
        "outputId": "e719cde2-f535-47f4-9a39-10fa0f656025"
      },
      "source": [
        "spm.SentencePieceTrainer.train(\n",
        "    '--input=en_sentences.txt --model_prefix=en_sp --vocab_size=16000 --model_type=bpe'\n",
        ")\n",
        "\n",
        "print(\"SentencePiece model for English (en_sp.model, en_sp.vocab) trained.\")"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentencePiece model for English (en_sp.model, en_sp.vocab) trained.\n"
          ]
        }
      ]
    }
  ]
}