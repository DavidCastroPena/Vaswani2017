{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODpWik6tyKxfwHTAYWKF0W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidCastroPena/Vaswani2017/blob/main/replicatingAttentionIsAllWhatYouNeed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnreNdFpCsLR",
        "outputId": "c797fe85-94d7-4cda-bbd2-644135653042"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# when we are using a transformer to analyze say a sentence (\"the cat sat\"), first what happens is that each word gets stored in a\n",
        "# query: q_sat. Now each query or what i am looking to analize is compared (dot product) with its all the keys; all words have keys: k_the, k_cat,\n",
        "# and k_sat. The dot product geometrically shows that if the direction of two vectors is related, this will reflect a large dot\n",
        "#product, which shows that the two tokens are related\n",
        "\n",
        "\"\"\"\n",
        "qsat‚ãÖkthe= 0.1\n",
        "\t‚Äã\n",
        "ùëûsat‚ãÖùëòcat=2.1\n",
        "\n",
        "qsat‚ãÖkcat=1.5\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# \"sat\" is more related to cat than the\n",
        "\n",
        "#Recall, the keys and query are linear transformation of their embeddings. Intuitively, we say that the query is a question\n",
        "#per token that aims to uncover the role of the specific word in a given text. The word ‚Äúquestion‚Äù is shorthand for\n",
        "#something very precise: The query defines a direction in vector space along which relevance is measured.\n",
        "\n",
        "\n",
        "\n",
        "#Lesson 1 goal: implement and understand:\n",
        "\n",
        "#Attention(Q,K,V)=softmax(QK^T/root(d_k)*V\n",
        "\n",
        "#Key concepts:\n",
        "\n",
        "#PyTorch tensors\n",
        "\n",
        "#Matrix multiplication\n",
        "\n",
        "#Softmax\n",
        "\n",
        "#Masking\n",
        "\n",
        "#Shape reasoning\n",
        "\n",
        "\n",
        "#Setup\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(0)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Q,K ,V, and a mask are tensor. A tensor is a multi-dimensional array of numbers and in transformers all the meaning\n",
        "#arises from learned tensor transformations and interactions\n",
        "\n",
        "# embeddings are vectors, but to make computations or to represent sentences, instead of converting to\n",
        "# vectors in a sequential way or one by one, tensors allow you to represent text in a more efficient way\n",
        "\n",
        "\n",
        "# A vector is a tensor (a 1-D tensor)\n",
        "\n",
        "# A matrix is a tensor (a 2-D tensor)\n",
        "\n",
        "# So the real question is:\n",
        "\n",
        "# Why do we need higher-dimensional tensors instead of just one vector at a time? Imagine you process one word at a time,\n",
        "# using vectors only. Sentence: ‚ÄúThe cat sat‚Äù\n",
        "\n",
        "# You would have to do this:\n",
        "\n",
        "# Take embedding of \"The\" ‚Üí vector\n",
        "\n",
        "# Compare it to \"cat\" ‚Üí vector\n",
        "\n",
        "# Compare it to \"sat\" ‚Üí vector\n",
        "\n",
        "# Repeat for \"cat\"\n",
        "\n",
        "# Repeat for \"sat\"\n",
        "\n",
        "# That‚Äôs:\n",
        "\n",
        "# nested loops\n",
        "\n",
        "# sequential computation\n",
        "\n",
        "# very slow\n",
        "\n",
        "# hard to parallelize\n",
        "\n",
        "# messy gradients\n",
        "\n",
        "# This is basically how early RNNs worked.\n",
        "\n",
        "# 3Ô∏è‚É£ What tensors give you: structure\n",
        "\n",
        "# Tensors let you represent many things at once.\n",
        "\n",
        "# Instead of:\n",
        "\n",
        "# ‚ÄúOne word ‚Üí one vector‚Äù\n",
        "\n",
        "# You represent:\n",
        "\n",
        "# All words, all positions, all heads, all batches ‚Äî at the same time\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Q: (batch, heads, q_len, d_k)\n",
        "    K: (batch, heads, k_len, d_k)\n",
        "    V: (batch, heads, k_len, d_v)\n",
        "    mask: (batch, heads, q_len, k_len) with 1 for allowed, 0 for blocked (optional)\n",
        "\n",
        "    returns:\n",
        "      out: (batch, heads, q_len, d_v)\n",
        "      attn: (batch, heads, q_len, k_len)\n",
        "    \"\"\"\n",
        "    d_k = Q.size(-1)\n",
        "\n",
        "    # (batch, heads, q_len, k_len)\n",
        "    scores = Q @ K.transpose(-2, -1) / (d_k ** 0.5)\n",
        "\n",
        "    if mask is not None:\n",
        "        # Set blocked positions to a large negative value so softmax ~ 0 there.\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    attn = F.softmax(scores, dim=-1)\n",
        "    out = attn @ V\n",
        "    return out, attn\n"
      ],
      "metadata": {
        "id": "0evCiQo3C6lY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets create a small example to see the attention at work\n",
        "\n",
        "batch, heads, seq_len, d_k, d_v = 1, 1, 4, 3, 2\n",
        "\n",
        "Q = torch.randn(batch, heads, seq_len, d_k, device=device)\n",
        "K = torch.randn(batch, heads, seq_len, d_k, device=device)\n",
        "V = torch.randn(batch, heads, seq_len, d_v, device=device)\n",
        "\n",
        "out, attn = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "print(\"Q shape:\", Q.shape)\n",
        "print(\"attn shape:\", attn.shape)\n",
        "print(\"out shape:\", out.shape)\n",
        "\n",
        "print(\"\\nAttention matrix (q_len x k_len):\\n\", attn[0,0].detach().cpu())\n",
        "print(\"\\nRow sums (should be ~1):\\n\", attn[0,0].sum(dim=-1).detach().cpu())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDKyBI-mL8VM",
        "outputId": "c37a6b70-cae5-459b-c831-7c67a34feb8e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q shape: torch.Size([1, 1, 4, 3])\n",
            "attn shape: torch.Size([1, 1, 4, 4])\n",
            "out shape: torch.Size([1, 1, 4, 2])\n",
            "\n",
            "Attention matrix (q_len x k_len):\n",
            " tensor([[0.3921, 0.2020, 0.0964, 0.3094],\n",
            "        [0.2307, 0.2815, 0.3649, 0.1228],\n",
            "        [0.4548, 0.1400, 0.0701, 0.3350],\n",
            "        [0.1593, 0.2621, 0.4593, 0.1193]])\n",
            "\n",
            "Row sums (should be ~1):\n",
            " tensor([1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    }
  ]
}